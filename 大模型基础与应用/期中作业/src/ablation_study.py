import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import json
import os
from datetime import datetime
from rouge_score import rouge_scorer
import math

from model import Transformer
from config import config
from data_loader import get_data_loaders
from utils import set_seed

import warnings
warnings.filterwarnings("ignore")
plt.rcParams["font.sans-serif"] = ["SimHei"]
plt.rcParams["axes.unicode_minus"] = False

class AblationTransformer(Transformer):
    """
    æ¶ˆèå®éªŒä¸“ç”¨Transformeræ¨¡å‹ï¼šæ”¯æŒå¯ç”¨/ç¦ç”¨ä½ç½®ç¼–ç åŠŸèƒ½
    è¿™ä¸ªç±»ç»§æ‰¿è‡ªåŸºç¡€çš„Transformeræ¨¡å‹ï¼Œé€šè¿‡æ§åˆ¶ä½ç½®ç¼–ç çš„ä½¿ç”¨ä¸å¦
    æ¥ç ”ç©¶ä½ç½®ä¿¡æ¯åœ¨å¯¹è¯æ‘˜è¦ä»»åŠ¡ä¸­çš„é‡è¦æ€§ã€‚
    æ¶ˆèå®éªŒ(Ablation Study)æ˜¯æœºå™¨å­¦ä¹ ä¸­å¸¸ç”¨çš„åˆ†ææ–¹æ³•ï¼Œé€šè¿‡
    ç§»é™¤æ¨¡å‹çš„æŸä¸ªç»„ä»¶æ¥è¯„ä¼°è¯¥ç»„ä»¶å¯¹æ•´ä½“æ€§èƒ½çš„è´¡çŒ®ã€‚
    ä½ç½®ç¼–ç (Positional Encoding)æ˜¯Transformeræ¶æ„çš„å…³é”®ç»„ä»¶ï¼Œ
    ä¸ºæ¨¡å‹æä¾›åºåˆ—ä¸­tokençš„ä½ç½®ä¿¡æ¯ï¼Œå› ä¸ºè‡ªæ³¨æ„åŠ›æœºåˆ¶æœ¬èº«æ˜¯ä½ç½®æ— å…³çš„ã€‚
    """
    def __init__(self, config, tokenizer=None, use_positional_encoding=True):
        """
        åˆå§‹åŒ–æ¶ˆèå®éªŒæ¨¡å‹
        Args:
            config: æ¨¡å‹é…ç½®å¯¹è±¡ï¼ŒåŒ…å«æ‰€æœ‰è¶…å‚æ•°è®¾ç½®
            tokenizer: åˆ†è¯å™¨å®ä¾‹ï¼Œç”¨äºæ–‡æœ¬ç¼–ç å’Œè§£ç 
                - æä¾›è¯æ±‡è¡¨å¤§å°ã€ç‰¹æ®Štoken IDç­‰ä¿¡æ¯
            use_positional_encoding: æ˜¯å¦ä½¿ç”¨ä½ç½®ç¼–ç ï¼ˆæ¶ˆèå˜é‡ï¼‰
                - True: ä½¿ç”¨æ ‡å‡†ä½ç½®ç¼–ç ï¼ˆåŸºçº¿æ¨¡å‹ï¼‰
                - False: ç¦ç”¨ä½ç½®ç¼–ç ï¼ˆæ¶ˆèæ¨¡å‹ï¼‰
        """
        # è°ƒç”¨çˆ¶ç±»æ„é€ å‡½æ•°ï¼Œåˆå§‹åŒ–åŸºç¡€Transformeræ¶æ„
        super().__init__(config, tokenizer)
        # å­˜å‚¨æ¶ˆèå®éªŒé…ç½®
        self.use_positional_encoding = use_positional_encoding

        # å¦‚æœç¦ç”¨ä½ç½®ç¼–ç ï¼Œå°†ä½ç½®ç¼–ç å±‚æ›¿æ¢ä¸ºæ’ç­‰æ˜ å°„
        if not use_positional_encoding:
            # nn.Identity()æ˜¯ä¸€ä¸ªæ’ç­‰æ˜ å°„å±‚ï¼Œè¾“å…¥ä»€ä¹ˆå°±è¾“å‡ºä»€ä¹ˆ
            # è¿™ç›¸å½“äºç§»é™¤äº†ä½ç½®ä¿¡æ¯ï¼Œåªä¿ç•™è¯åµŒå…¥ä¿¡æ¯
            self.pos_encoding = nn.Identity()  # æ’ç­‰æ˜ å°„ï¼Œä¸æ·»åŠ ä½ç½®ä¿¡æ¯
            # åŸå§‹ä½ç½®ç¼–ç ï¼šx = word_embedding + positional_encoding
            # æ¶ˆèç‰ˆæœ¬ï¼šx = word_embedding + Identity(word_embedding) = word_embedding
            # è¿™æ ·ç¡®ä¿äº†è¾“å…¥ç»´åº¦çš„ä¸€è‡´æ€§ï¼Œä½†ç§»é™¤äº†ä½ç½®ä¿¡æ¯

    def encode(self, src, src_mask=None):
        """
        é‡å†™ç¼–ç æ–¹æ³•ï¼Œæ”¯æŒä½ç½®ç¼–ç å¼€å…³
        ç¼–ç å™¨è´Ÿè´£å¤„ç†è¾“å…¥åºåˆ—ï¼ˆå¯¹è¯æ–‡æœ¬ï¼‰ï¼Œæå–è¯­ä¹‰è¡¨ç¤ºã€‚
        è¿™ä¸ªæ–¹æ³•çš„ä¿®æ”¹å…è®¸æˆ‘ä»¬æ§åˆ¶æ˜¯å¦æ·»åŠ ä½ç½®ä¿¡æ¯ã€‚
        Args:
            src: æºåºåˆ—token IDsï¼Œå½¢çŠ¶ä¸º(batch_size, src_seq_len)
                åŒ…å«å¯¹è¯æ–‡æœ¬çš„tokenizedè¡¨ç¤º
            src_mask: æºåºåˆ—æ©ç ï¼Œå½¢çŠ¶ä¸º(batch_size, 1, 1, src_seq_len)
                ç”¨äºæ ‡è¯†å“ªäº›ä½ç½®æ˜¯å¡«å……tokenï¼ˆéœ€è¦è¢«å¿½ç•¥ï¼‰
        Returns:
            tuple: (encoder_output, encoder_self_attentions)
                - encoder_output: ç¼–ç å™¨è¾“å‡ºï¼Œå½¢çŠ¶ä¸º(batch_size, src_seq_len, d_model)
                  åŒ…å«è¾“å…¥åºåˆ—çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥è¡¨ç¤º
                - encoder_self_attentions: å„å±‚è‡ªæ³¨æ„åŠ›æƒé‡åˆ—è¡¨
                  ç”¨äºå¯è§†åŒ–å’Œåˆ†ææ¨¡å‹å…³æ³¨ç‚¹
        """
        # å¦‚æœæ²¡æœ‰æä¾›æ©ç ï¼Œè‡ªåŠ¨åˆ›å»ºï¼ˆå¿½ç•¥å¡«å……tokenï¼‰
        if src_mask is None:
            src_mask = self._create_src_mask(src)

        # 1. è¯åµŒå…¥ï¼šå°†ç¦»æ•£çš„token IDè½¬æ¢ä¸ºè¿ç»­çš„å‘é‡è¡¨ç¤º
        # å½¢çŠ¶: (batch_size, seq_len) -> (batch_size, seq_len, d_model)
        x = self.embedding(src)

        # 2. æ¡ä»¶ä½ç½®ç¼–ç ï¼šæ ¹æ®æ¶ˆèå®éªŒè®¾ç½®å†³å®šæ˜¯å¦æ·»åŠ ä½ç½®ä¿¡æ¯
        # è¿™æ˜¯æ¶ˆèå®éªŒçš„æ ¸å¿ƒä¿®æ”¹ç‚¹
        if self.use_positional_encoding:
            # ä½¿ç”¨ä½ç½®ç¼–ç ï¼šæ·»åŠ æ­£å¼¦/ä½™å¼¦ä½ç½®ä¿¡æ¯
            # è®©æ¨¡å‹èƒ½å¤Ÿæ„ŸçŸ¥tokenåœ¨åºåˆ—ä¸­çš„ä½ç½®
            x = self.pos_encoding(x)
        # else:
        #   ä¸ä½¿ç”¨ä½ç½®ç¼–ç ï¼šä¿æŒåŸå§‹çš„è¯è¯­ä¹‰åµŒå…¥
        #   æ¨¡å‹åªèƒ½åŸºäºå†…å®¹ä¿¡æ¯ï¼Œæ— æ³•æ„ŸçŸ¥é¡ºåºå…³ç³»

        # 3. åº”ç”¨dropoutè¿›è¡Œæ­£åˆ™åŒ–ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
        x = self.dropout(x)

        # 4. é€šè¿‡å¤šå±‚ç¼–ç å™¨è¿›è¡Œå¤„ç†
        encoder_self_attentions = []
        for layer in self.encoder_layers:
            # æ¯å±‚åŒ…å«ï¼šè‡ªæ³¨æ„åŠ› + å‰é¦ˆç½‘ç»œ + æ®‹å·®è¿æ¥ + å±‚å½’ä¸€åŒ–
            x, self_attn = layer(x, src_mask)
            # å­˜å‚¨æ³¨æ„åŠ›æƒé‡ç”¨äºåˆ†æ
            encoder_self_attentions.append(self_attn)

        return x, encoder_self_attentions

    def decode(self, tgt, encoder_output, src_mask=None, tgt_mask=None):
        """
        é‡å†™è§£ç æ–¹æ³•ï¼Œæ”¯æŒä½ç½®ç¼–ç å¼€å…³
        è§£ç å™¨åŸºäºç¼–ç å™¨è¾“å‡ºå’Œå·²ç”Ÿæˆçš„éƒ¨åˆ†åºåˆ—ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ªtokenã€‚
        åŒæ ·éœ€è¦æ§åˆ¶ä½ç½®ä¿¡æ¯çš„ä½¿ç”¨ã€‚
        Args:
            tgt: ç›®æ ‡åºåˆ—token IDsï¼ˆå·²ç”Ÿæˆçš„éƒ¨åˆ†ï¼‰ï¼Œå½¢çŠ¶ä¸º(batch_size, tgt_seq_len)
                é€šå¸¸æ¯”æºåºåˆ—çŸ­ï¼ŒåŒ…å«æ‘˜è¦æ–‡æœ¬çš„tokenizedè¡¨ç¤º
            encoder_output: ç¼–ç å™¨è¾“å‡ºï¼Œå½¢çŠ¶ä¸º(batch_size, src_seq_len, d_model)
                åŒ…å«è¾“å…¥åºåˆ—çš„è¯­ä¹‰ä¿¡æ¯
            src_mask: æºåºåˆ—æ©ç ï¼Œå½¢çŠ¶ä¸º(batch_size, 1, 1, src_seq_len)
                é˜²æ­¢å…³æ³¨åˆ°å¡«å……token
            tgt_mask: ç›®æ ‡åºåˆ—æ©ç ï¼Œå½¢çŠ¶ä¸º(batch_size, 1, tgt_seq_len, tgt_seq_len)
                å› æœæ©ç ï¼Œé˜²æ­¢çœ‹åˆ°æœªæ¥ä¿¡æ¯ï¼ˆç¡®ä¿è‡ªå›å½’æ€§è´¨ï¼‰
        Returns:
            tuple: (logits, decoder_self_attentions, decoder_cross_attentions)
                - logits: æ¨¡å‹è¾“å‡ºåˆ†æ•°ï¼Œå½¢çŠ¶ä¸º(batch_size, tgt_seq_len, vocab_size)
                  æ¯ä¸ªä½ç½®å¯¹è¯æ±‡è¡¨ä¸­æ‰€æœ‰tokençš„æœªå½’ä¸€åŒ–åˆ†æ•°
                - decoder_self_attentions: è§£ç å™¨è‡ªæ³¨æ„åŠ›æƒé‡åˆ—è¡¨
                  æ˜¾ç¤ºè§£ç å™¨å¦‚ä½•å…³æ³¨å·²ç”Ÿæˆçš„ç›®æ ‡åºåˆ—éƒ¨åˆ†
                - decoder_cross_attentions: è§£ç å™¨-ç¼–ç å™¨äº¤å‰æ³¨æ„åŠ›æƒé‡åˆ—è¡¨
                  æ˜¾ç¤ºè§£ç å™¨å¦‚ä½•å…³æ³¨æºåºåˆ—çš„ä¸åŒéƒ¨åˆ†ï¼ˆç±»ä¼¼å¯¹é½æœºåˆ¶ï¼‰
        """
        # å¦‚æœæ²¡æœ‰æä¾›ç›®æ ‡æ©ç ï¼Œè‡ªåŠ¨åˆ›å»ºå› æœæ©ç 
        if tgt_mask is None:
            tgt_mask = self._create_tgt_mask(tgt.size(1))

        # 1. è¯åµŒå…¥ï¼šç›®æ ‡åºåˆ—çš„è¿ç»­å‘é‡è¡¨ç¤º
        # å½¢çŠ¶: (batch_size, seq_len) -> (batch_size, seq_len, d_model)
        x = self.embedding(tgt)

        # 2. æ¡ä»¶ä½ç½®ç¼–ç ï¼šåŒæ ·æ ¹æ®æ¶ˆèè®¾ç½®å†³å®š
        if self.use_positional_encoding:
            # ä½¿ç”¨ä½ç½®ç¼–ç ï¼šè®©è§£ç å™¨æ„ŸçŸ¥ç”Ÿæˆé¡ºåº
            x = self.pos_encoding(x)
        # else:
        #   ä¸ä½¿ç”¨ä½ç½®ç¼–ç ï¼šè§£ç å™¨åªèƒ½åŸºäºå†…å®¹ç”Ÿæˆï¼Œæ— æ³•æ„ŸçŸ¥ç”Ÿæˆé¡ºåº

        # 3. åº”ç”¨dropout
        x = self.dropout(x)

        # 4. é€šè¿‡å¤šå±‚è§£ç å™¨è¿›è¡Œå¤„ç†
        decoder_self_attentions = []  # å­˜å‚¨æ¯å±‚çš„è‡ªæ³¨æ„åŠ›æƒé‡
        decoder_cross_attentions = []  # å­˜å‚¨æ¯å±‚çš„äº¤å‰æ³¨æ„åŠ›æƒé‡
        for layer in self.decoder_layers:
            # æ¯å±‚åŒ…å«ï¼šæ©ç è‡ªæ³¨æ„åŠ› + ç¼–ç å™¨-è§£ç å™¨æ³¨æ„åŠ› + å‰é¦ˆç½‘ç»œ + æ®‹å·®è¿æ¥ + å±‚å½’ä¸€åŒ–
            x, self_attn, cross_attn = layer(x, encoder_output, src_mask, tgt_mask)
            decoder_self_attentions.append(self_attn)
            decoder_cross_attentions.append(cross_attn)

        # 5. è¾“å‡ºæŠ•å½±ï¼šå°†éšè—è¡¨ç¤ºæ˜ å°„å›è¯æ±‡è¡¨ç©ºé—´
        # å½¢çŠ¶: (batch_size, seq_len, d_model) -> (batch_size, seq_len, vocab_size)
        logits = self.output_projection(x)

        return logits, decoder_self_attentions, decoder_cross_attentions


class AblationStudy:
    """
    ä½ç½®ç¼–ç æ¶ˆèå®éªŒç±»
    æ¯”è¾ƒä½¿ç”¨ä½ç½®ç¼–ç  vs ä¸ä½¿ç”¨ä½ç½®ç¼–ç çš„æ¨¡å‹æ€§èƒ½å·®å¼‚
    æ¶ˆèå®éªŒ(Ablation Study)æ˜¯æœºå™¨å­¦ä¹ ä¸­é‡è¦çš„åˆ†ææ–¹æ³•ï¼Œé€šè¿‡
    ç§»é™¤æ¨¡å‹çš„æŸä¸ªç»„ä»¶æ¥è¯„ä¼°è¯¥ç»„ä»¶å¯¹æ•´ä½“æ€§èƒ½çš„è´¡çŒ®ã€‚
    æœ¬å®éªŒä¸“é—¨ç ”ç©¶ä½ç½®ç¼–ç (Positional Encoding)åœ¨Transformer
    å¯¹è¯æ‘˜è¦ä»»åŠ¡ä¸­çš„é‡è¦æ€§ã€‚
    å®éªŒè®¾è®¡ï¼š
    - åŸºçº¿æ¨¡å‹ï¼šä½¿ç”¨æ ‡å‡†ä½ç½®ç¼–ç ï¼ˆå®Œæ•´Transformerï¼‰
    - æ¶ˆèæ¨¡å‹ï¼šç§»é™¤ä½ç½®ç¼–ç åŠŸèƒ½ï¼ˆä½¿ç”¨æ’ç­‰æ˜ å°„ä»£æ›¿ï¼‰
    - å¯¹æ¯”æŒ‡æ ‡ï¼šæŸå¤±ã€å›°æƒ‘åº¦ã€å‡†ç¡®ç‡ã€ROUGEåˆ†æ•°
    """
    def __init__(self, config, tokenizer, train_loader, val_loader, test_loader):
        """
        åˆå§‹åŒ–æ¶ˆèå®éªŒ
        Args:
            config: å®éªŒé…ç½®å¯¹è±¡ï¼ŒåŒ…å«æ‰€æœ‰è¶…å‚æ•°è®¾ç½®
                - device: è®¡ç®—è®¾å¤‡ (cuda/cpu)
                - num_epochs: è®­ç»ƒè½®æ•°
                - batch_size: æ‰¹æ¬¡å¤§å°
                - learning_rate: å­¦ä¹ ç‡
                - max_grad_norm: æ¢¯åº¦è£å‰ªé˜ˆå€¼
                - max_target_length: ç›®æ ‡åºåˆ—æœ€å¤§é•¿åº¦
                - n_heads: æ³¨æ„åŠ›å¤´æ•°é‡
                - d_model: æ¨¡å‹ç»´åº¦
            tokenizer: åˆ†è¯å™¨å®ä¾‹ï¼Œç”¨äºæ–‡æœ¬ç¼–ç å’Œè§£ç 
                - vocab_size: è¯æ±‡è¡¨å¤§å°
                - pad_token_id: å¡«å……token ID
                - æä¾›encode/decodeåŠŸèƒ½
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨ï¼ŒåŒ…å«è®­ç»ƒæ•°æ®é›†
                - ç”¨äºæ¨¡å‹å‚æ•°æ›´æ–°
                - æ‰¹æ¬¡æ•°æ®æ ¼å¼ï¼š{'input_ids': ..., 'attention_mask': ..., 'labels': ...}
            val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨ï¼ŒåŒ…å«éªŒè¯æ•°æ®é›†
                - ç”¨äºæ¨¡å‹æ€§èƒ½è¯„ä¼°å’Œé€‰æ‹©
                - ç»“æ„ä¸è®­ç»ƒåŠ è½½å™¨ç›¸åŒ
            test_loader: æµ‹è¯•æ•°æ®åŠ è½½å™¨ï¼ŒåŒ…å«æµ‹è¯•æ•°æ®é›†
                - ç”¨äºæœ€ç»ˆæ€§èƒ½è¯„ä¼°ï¼ˆæœ¬å®éªŒæœªä½¿ç”¨ï¼‰
                - ç»“æ„ä¸è®­ç»ƒåŠ è½½å™¨ç›¸åŒ
        å®éªŒè®¾ç½®è¯´æ˜ï¼š
        - ä¿æŒæ‰€æœ‰å…¶ä»–è¶…å‚æ•°ç›¸åŒï¼Œåªæ”¹å˜ä½ç½®ç¼–ç çš„ä½¿ç”¨
        - ä½¿ç”¨ç›¸åŒçš„éšæœºç§å­ç¡®ä¿å¯é‡å¤æ€§
        - åœ¨ç›¸åŒçš„ç¡¬ä»¶ç¯å¢ƒä¸‹è¿è¡Œä¸¤ä¸ªå®éªŒ
        """
        self.config = config
        self.config.num_epochs = 10# ä¸ºäº†èŠ‚çœæ—¶é—´ï¼Œå°†è®­ç»ƒè½®æ•°é™åˆ¶ä¸º10
        self.tokenizer = tokenizer
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.test_loader = test_loader
        self.device = config.device  # è®¡ç®—è®¾å¤‡ï¼ˆGPU/CPUï¼‰

        # åˆ›å»ºç»“æœç›®å½•
        self.results_dir = "../results/ablation_study"
        os.makedirs(self.results_dir, exist_ok=True)
        # ç›®å½•ç»“æ„ï¼š
        # ../results/ablation_study/
        #   â”œâ”€â”€ ablation_results_10epochs_8heads.json    # JSONæ ¼å¼å®Œæ•´ç»“æœ
        #   â”œâ”€â”€ experiment_summary_10epochs_8heads.txt    # æ–‡æœ¬æ‘˜è¦
        #   â””â”€â”€ ablation_comparison_10epochs_8heads.png  # å¯è§†åŒ–å›¾è¡¨

        # å®éªŒè®°å½• - ç»“æ„åŒ–å­˜å‚¨æ‰€æœ‰å®éªŒç»“æœ
        self.results = {
            "experiment_info": {
                "timestamp": datetime.now().isoformat(),  # å®éªŒæ—¶é—´æˆ³ï¼ˆISOæ ¼å¼ï¼‰
                "config": config.__dict__,  # é…ç½®å‚æ•°ï¼ˆè½¬æ¢ä¸ºå­—å…¸ä¾¿äºåºåˆ—åŒ–ï¼‰
                "description": "ä½ç½®ç¼–ç æ¶ˆèå®éªŒï¼šæ¯”è¾ƒä½¿ç”¨ä½ç½®ç¼–ç  vs ä¸ä½¿ç”¨ä½ç½®ç¼–ç çš„æ¨¡å‹æ€§èƒ½"
            },
            "with_positional_encoding": {},  # åŸºçº¿æ¨¡å‹ç»“æœï¼ˆæœ‰ä½ç½®ç¼–ç ï¼‰
            "without_positional_encoding": {}  # æ¶ˆèæ¨¡å‹ç»“æœï¼ˆæ— ä½ç½®ç¼–ç ï¼‰
        }

    def calculate_perplexity(self, loss):
        """
        è®¡ç®—å›°æƒ‘åº¦(Perplexity)
        å›°æƒ‘åº¦æ˜¯è¯­è¨€æ¨¡å‹ä¸­æœ€é‡è¦çš„è¯„ä¼°æŒ‡æ ‡ä¹‹ä¸€ï¼Œè¡¨ç¤ºæ¨¡å‹å¯¹æµ‹è¯•æ•°æ®çš„"å›°æƒ‘ç¨‹åº¦"ã€‚
        æ•°å­¦å…¬å¼ï¼šperplexity = exp(loss)
        Args:
            loss: äº¤å‰ç†µæŸå¤±å€¼
                - ç±»å‹: float
                - èŒƒå›´: 0åˆ°æ­£æ— ç©·ï¼ˆè¶Šå°è¶Šå¥½ï¼‰
                - æ¥æº: äº¤å‰ç†µæŸå¤±å‡½æ•°è®¡ç®—ç»“æœ
        Returns:
            float: å›°æƒ‘åº¦å€¼
                - èŒƒå›´: 1åˆ°æ­£æ— ç©·ï¼ˆè¶Šå°è¶Šå¥½ï¼‰
                - å®Œç¾é¢„æµ‹: å›°æƒ‘åº¦ = 1ï¼ˆæŸå¤±ä¸º0æ—¶ï¼‰
                - éšæœºçŒœæµ‹: å›°æƒ‘åº¦ â‰ˆ è¯æ±‡è¡¨å¤§å°ï¼ˆæœ€å·®æƒ…å†µï¼‰
        è§£é‡Šï¼š
        - å›°æƒ‘åº¦å¯ä»¥ç†è§£ä¸º"æ¨¡å‹éœ€è¦å¹³å‡è€ƒè™‘å¤šå°‘ä¸ªå€™é€‰è¯"
        - å€¼è¶Šå°è¡¨ç¤ºæ¨¡å‹å¯¹æ•°æ®çš„é¢„æµ‹è¶Šç¡®å®šå’Œå‡†ç¡®
        - åœ¨è¯­è¨€æ¨¡å‹ä¸­ï¼Œå›°æƒ‘åº¦é€šå¸¸æ¯”æŸå¤±å‡½æ•°æ›´ç›´è§‚
        """
        return math.exp(loss)

    def calculate_accuracy(self, logits, labels, ignore_index=-100):
        """
        è®¡ç®—å‡†ç¡®ç‡(Accuracy)
        å‡†ç¡®ç‡è¡¡é‡æ¨¡å‹é¢„æµ‹æ­£ç¡®çš„tokenæ¯”ä¾‹ï¼Œæ˜¯ç›´è§‚çš„è¯„ä¼°æŒ‡æ ‡ã€‚
        åªè®¡ç®—éå¡«å……ä½ç½®çš„é¢„æµ‹å‡†ç¡®æ€§ã€‚
        Args:
            logits: æ¨¡å‹è¾“å‡ºï¼Œå½¢çŠ¶ä¸º(batch_size, seq_len, vocab_size)
                - æ¯ä¸ªä½ç½®å¯¹è¯æ±‡è¡¨ä¸­æ‰€æœ‰tokençš„æœªå½’ä¸€åŒ–åˆ†æ•°
                - é€šè¿‡softmaxå¯ä»¥è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒ
            labels: çœŸå®æ ‡ç­¾ï¼Œå½¢çŠ¶ä¸º(batch_size, seq_len)
                - ç›®æ ‡åºåˆ—çš„token ID
                - åŒ…å«ç‰¹æ®Štokenï¼ˆå¦‚å¡«å……tokenã€èµ·å§‹ç¬¦ã€ç»“æŸç¬¦ï¼‰
            ignore_index: è¦å¿½ç•¥çš„æ ‡ç­¾ç´¢å¼•
                - é»˜è®¤å€¼: -100ï¼ˆPyTorchæ ‡å‡†å¿½ç•¥ç´¢å¼•ï¼‰
                - å®é™…ä½¿ç”¨: self.tokenizer.pad_token_idï¼ˆå¡«å……tokenï¼‰
                - ä½œç”¨: é¿å…å¡«å……ä½ç½®å½±å“å‡†ç¡®ç‡è®¡ç®—
        Returns:
            float: å‡†ç¡®ç‡ï¼ˆ0.0åˆ°1.0ä¹‹é—´ï¼‰
                - 0.0: æ‰€æœ‰é¢„æµ‹éƒ½é”™è¯¯
                - 0.5: ä¸€åŠé¢„æµ‹æ­£ç¡®
                - 1.0: æ‰€æœ‰é¢„æµ‹éƒ½æ­£ç¡®
        è®¡ç®—è¿‡ç¨‹ï¼š
        1. è·å–é¢„æµ‹ç»“æœï¼ˆæ¦‚ç‡æœ€å¤§çš„tokenï¼‰
        2. åˆ›å»ºæœ‰æ•ˆtokenæ©ç ï¼ˆå¿½ç•¥å¡«å……ä½ç½®ï¼‰
        3. è®¡ç®—æ­£ç¡®é¢„æµ‹çš„æ•°é‡
        4. è®¡ç®—æœ‰æ•ˆtokençš„æ€»æ•°
        5. è®¡ç®—å‡†ç¡®ç‡ = æ­£ç¡®æ•°é‡ / æœ‰æ•ˆæ€»æ•°
        """
        # 1. è·å–é¢„æµ‹ç»“æœ - é€‰æ‹©æ¦‚ç‡æœ€å¤§çš„token
        # torch.argmaxè¿”å›æ¯ä¸ªä½ç½®æœ€å¤§å€¼çš„ç´¢å¼•
        # dim=-1è¡¨ç¤ºåœ¨æœ€åä¸€ä¸ªç»´åº¦ï¼ˆvocab_sizeï¼‰ä¸Šå–æœ€å¤§å€¼
        predictions = torch.argmax(logits, dim=-1)
        # 2. åˆ›å»ºæœ‰æ•ˆtokençš„æ©ç  - æ ‡è¯†å“ªäº›ä½ç½®éœ€è¦å‚ä¸è®¡ç®—
        # å¿½ç•¥å¡«å……tokenå’Œå…¶ä»–æŒ‡å®šå¿½ç•¥çš„token
        valid_mask = (labels != ignore_index)
        # 3. è®¡ç®—æ­£ç¡®é¢„æµ‹çš„æ•°é‡
        # æ¡ä»¶1: é¢„æµ‹ç­‰äºçœŸå®æ ‡ç­¾
        # æ¡ä»¶2: ä½ç½®æ˜¯æœ‰æ•ˆçš„ï¼ˆéå¡«å……ï¼‰
        correct = (predictions == labels) & valid_mask
        correct_count = correct.sum().item()  # ç»Ÿè®¡Trueçš„æ•°é‡
        # 4. è®¡ç®—æœ‰æ•ˆtokençš„æ€»æ•°
        total_valid = valid_mask.sum().item()  # ç»Ÿè®¡æœ‰æ•ˆä½ç½®æ•°é‡
        # 5. é¿å…é™¤é›¶é”™è¯¯å¹¶è®¡ç®—å‡†ç¡®ç‡
        if total_valid == 0:
            return 0.0  # å¦‚æœæ²¡æœ‰æœ‰æ•ˆtokenï¼Œè¿”å›0å‡†ç¡®ç‡
        accuracy = correct_count / total_valid
        return accuracy

    def train_model(self, use_positional_encoding=True, model_name="baseline"):
        """
        è®­ç»ƒå•ä¸ªæ¨¡å‹ï¼š
        è¿™ä¸ªæ–¹æ³•æ˜¯æ¶ˆèå®éªŒçš„æ ¸å¿ƒï¼Œè´Ÿè´£è®­ç»ƒä¸€ä¸ªå®Œæ•´çš„Transformeræ¨¡å‹ï¼Œ
        å¹¶æ ¹æ®æ¶ˆèè®¾ç½®æ§åˆ¶æ˜¯å¦ä½¿ç”¨ä½ç½®ç¼–ç ã€‚
        Args:
            use_positional_encoding: æ˜¯å¦ä½¿ç”¨ä½ç½®ç¼–ç ï¼ˆæ¶ˆèå˜é‡ï¼‰
                - True: ä½¿ç”¨æ ‡å‡†ä½ç½®ç¼–ç ï¼ˆåŸºçº¿æ¨¡å‹ï¼‰
                - False: ä¸ä½¿ç”¨ä½ç½®ç¼–ç ï¼ˆæ¶ˆèæ¨¡å‹ï¼‰
            model_name: æ¨¡å‹åç§°æ ‡è¯†ï¼Œç”¨äºæ—¥å¿—è®°å½•å’Œç»“æœå­˜å‚¨
                - ç¤ºä¾‹: "with_positional_encoding", "without_positional_encoding"
        Returns:
            dict: åŒ…å«å®Œæ•´è®­ç»ƒç»“æœå’Œæ¨¡å‹çŠ¶æ€çš„å­—å…¸
                - model: è®­ç»ƒå®Œæˆçš„æ¨¡å‹å®ä¾‹
                - train_losses: æ¯ä¸ªepochçš„å¹³å‡è®­ç»ƒæŸå¤±åˆ—è¡¨
                - val_losses: æ¯ä¸ªepochçš„å¹³å‡éªŒè¯æŸå¤±åˆ—è¡¨
                - train_perplexities: æ¯ä¸ªepochçš„å¹³å‡è®­ç»ƒå›°æƒ‘åº¦åˆ—è¡¨
                - val_perplexities: æ¯ä¸ªepochçš„å¹³å‡éªŒè¯å›°æƒ‘åº¦åˆ—è¡¨
                - train_accuracies: æ¯ä¸ªepochçš„å¹³å‡è®­ç»ƒå‡†ç¡®ç‡åˆ—è¡¨
                - val_accuracies: æ¯ä¸ªepochçš„å¹³å‡éªŒè¯å‡†ç¡®ç‡åˆ—è¡¨
                - rouge_scores: æ¯ä¸ªepochçš„ROUGEåˆ†æ•°åˆ—è¡¨
                - best_accuracy: æœ€ä½³éªŒè¯å‡†ç¡®ç‡
                - best_epoch: æœ€ä½³å‡†ç¡®ç‡å‡ºç°çš„epoch
                - best_model_state: æœ€ä½³æ¨¡å‹çŠ¶æ€å­—å…¸ï¼ˆç”¨äºä¿å­˜å’Œæ¢å¤ï¼‰
        """
        print(f"\n{'=' * 60}")
        print(f"è®­ç»ƒæ¨¡å‹: {model_name}")
        print(f"ä½¿ç”¨ä½ç½®ç¼–ç : {use_positional_encoding}")
        print(f"{'=' * 60}")

        # åˆ›å»ºæ¨¡å‹ - ä½¿ç”¨æ¶ˆèå®éªŒä¸“ç”¨Transformer
        model = AblationTransformer(
            self.config,
            self.tokenizer,
            use_positional_encoding=use_positional_encoding
        ).to(self.device)  # å°†æ¨¡å‹ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡ï¼ˆGPU/CPUï¼‰

        # ä¼˜åŒ–å™¨ - AdamW (Adam + æƒé‡è¡°å‡)
        # AdamWç»“åˆäº†Adamçš„è‡ªé€‚åº”å­¦ä¹ ç‡å’Œæƒé‡è¡°å‡æ­£åˆ™åŒ–
        optimizer = optim.AdamW(model.parameters(), lr=self.config.learning_rate)

        # å­¦ä¹ ç‡è°ƒåº¦å™¨ - ä½™å¼¦é€€ç«
        # æŒ‰ç…§ä½™å¼¦å‡½æ•°è°ƒæ•´å­¦ä¹ ç‡ï¼Œä»åˆå§‹å€¼è¡°å‡åˆ°0
        # T_maxå‚æ•°æŒ‡å®šä½™å¼¦å‘¨æœŸçš„é•¿åº¦ï¼ˆæ€»è®­ç»ƒè½®æ•°ï¼‰
        scheduler = optim.lr_scheduler.CosineAnnealingLR(
            optimizer, T_max=self.config.num_epochs
        )

        # æŸå¤±å‡½æ•° - äº¤å‰ç†µæŸå¤±
        # ignore_indexå‚æ•°å¿½ç•¥å¡«å……tokenï¼Œé¿å…å¡«å……ä½ç½®å½±å“æŸå¤±è®¡ç®—
        criterion = nn.CrossEntropyLoss(ignore_index=self.tokenizer.pad_token_id)

        # è®­ç»ƒè®°å½• - åˆå§‹åŒ–æ‰€æœ‰è·Ÿè¸ªæŒ‡æ ‡
        train_losses = []  # è®­ç»ƒæŸå¤±å†å²ï¼ˆæ¯ä¸ªepochçš„å¹³å‡å€¼ï¼‰
        val_losses = []  # éªŒè¯æŸå¤±å†å²
        train_perplexities = []  # è®­ç»ƒå›°æƒ‘åº¦å†å²
        val_perplexities = []  # éªŒè¯å›°æƒ‘åº¦å†å²
        train_accuracies = []  # è®­ç»ƒå‡†ç¡®ç‡å†å²
        val_accuracies = []  # éªŒè¯å‡†ç¡®ç‡å†å²
        rouge_scores = []  # ROUGEåˆ†æ•°å†å²ï¼ˆæ¯ä¸ªepochï¼‰

        # åŸºäºå‡†ç¡®ç‡é€‰æ‹©æœ€ä½³æ¨¡å‹
        best_accuracy = 0.0  # è®°å½•æœ€ä½³éªŒè¯å‡†ç¡®ç‡
        best_model_state = None  # è®°å½•æœ€ä½³æ¨¡å‹çŠ¶æ€ï¼ˆç”¨äºä¿å­˜ï¼‰
        best_epoch = 0  # è®°å½•æœ€ä½³å‡†ç¡®ç‡å‡ºç°çš„epoch

        # è®­ç»ƒå¾ªç¯ - éå†æ‰€æœ‰epoch
        for epoch in range(self.config.num_epochs):
            print(f"\nEpoch {epoch + 1}/{self.config.num_epochs}")

            # ==================== è®­ç»ƒé˜¶æ®µ ====================
            model.train()  # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼ï¼ˆå¯ç”¨dropoutã€batch normalizationç­‰ï¼‰
            total_train_loss = 0
            total_train_perplexity = 0
            total_train_accuracy = 0

            # ä½¿ç”¨tqdmè¿›åº¦æ¡æ˜¾ç¤ºè®­ç»ƒè¿›åº¦
            progress_bar = tqdm(self.train_loader, desc=f'è®­ç»ƒ {model_name}')

            # éå†è®­ç»ƒæ•°æ®çš„æ‰€æœ‰æ‰¹æ¬¡
            for batch in progress_bar:
                # å‡†å¤‡æ•°æ® - å°†æ•°æ®ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
                input_ids = batch['input_ids'].to(self.device)  # è¾“å…¥åºåˆ—token IDs
                labels = batch['labels'].to(self.device)  # ç›®æ ‡åºåˆ—token IDs

                # å‰å‘ä¼ æ’­ - ä½¿ç”¨Teacher ForcingæŠ€æœ¯
                # Teacher Forcing: ä½¿ç”¨çœŸå®ç›®æ ‡åºåˆ—ä½œä¸ºè§£ç å™¨è¾“å…¥ï¼ŒåŠ é€Ÿæ”¶æ•›
                # labels[:, :-1] ç§»é™¤ç›®æ ‡åºåˆ—çš„æœ€åä¸€ä¸ªtokenï¼Œä½œä¸ºè§£ç å™¨è¾“å…¥
                # è¿™æ ·æ¨¡å‹å­¦ä¹ çš„æ˜¯æ ¹æ®å‰n-1ä¸ªtokené¢„æµ‹ç¬¬nä¸ªtoken
                outputs = model(input_ids, labels[:, :-1])
                logits = outputs['logits']  # æ¨¡å‹è¾“å‡ºçš„åŸå§‹åˆ†æ•°

                # è®¡ç®—æŸå¤± - äº¤å‰ç†µæŸå¤±
                # å°†logitsé‡å¡‘ä¸ºäºŒç»´ï¼š(batch_size * seq_len, vocab_size)
                # å°†labelsé‡å¡‘ä¸ºä¸€ç»´ï¼š(batch_size * seq_len)
                # labels[:, 1:] ç§»é™¤ç¬¬ä¸€ä¸ªtokenï¼ˆé€šå¸¸æ˜¯èµ·å§‹ç¬¦ï¼‰
                loss = criterion(
                    logits.reshape(-1, logits.size(-1)),
                    labels[:, 1:].reshape(-1)
                )

                # è®¡ç®—å‡†ç¡®ç‡ - æ­£ç¡®é¢„æµ‹çš„tokenæ¯”ä¾‹
                accuracy = self.calculate_accuracy(
                    logits,
                    labels[:, 1:],  # å¿½ç•¥ç¬¬ä¸€ä¸ªtoken
                    ignore_index=self.tokenizer.pad_token_id
                )

                # åå‘ä¼ æ’­
                optimizer.zero_grad()  # æ¸…ç©ºä¸Šä¸€è½®çš„æ¢¯åº¦
                loss.backward()  # åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦
                # æ¢¯åº¦è£å‰ª - é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
                # å½“æ¢¯åº¦èŒƒæ•°è¶…è¿‡é˜ˆå€¼æ—¶ï¼Œå°†æ¢¯åº¦æŒ‰æ¯”ä¾‹ç¼©å°
                torch.nn.utils.clip_grad_norm_(model.parameters(), self.config.max_grad_norm)
                optimizer.step()  # æ ¹æ®æ¢¯åº¦æ›´æ–°æ¨¡å‹å‚æ•°

                # ç´¯è®¡ç»Ÿè®¡ - ç”¨äºè®¡ç®—epochå¹³å‡å€¼
                total_train_loss += loss.item()
                total_train_perplexity += self.calculate_perplexity(loss.item())
                total_train_accuracy += accuracy

                # æ›´æ–°è¿›åº¦æ¡æ˜¾ç¤º - å®æ—¶æ˜¾ç¤ºå½“å‰æ‰¹æ¬¡çš„æŒ‡æ ‡
                progress_bar.set_postfix({
                    'loss': f'{loss.item():.4f}',  # å½“å‰æ‰¹æ¬¡æŸå¤±
                    'acc': f'{accuracy:.4f}',  # å½“å‰æ‰¹æ¬¡å‡†ç¡®ç‡
                    'ppl': f'{self.calculate_perplexity(loss.item()):.2f}'  # å½“å‰æ‰¹æ¬¡å›°æƒ‘åº¦
                })

            # è®¡ç®—è®­ç»ƒå¹³å‡å€¼ - æ•´ä¸ªepochçš„å¹³å‡å€¼
            avg_train_loss = total_train_loss / len(self.train_loader)
            avg_train_perplexity = total_train_perplexity / len(self.train_loader)
            avg_train_accuracy = total_train_accuracy / len(self.train_loader)

            # è®°å½•è®­ç»ƒç»“æœ
            train_losses.append(avg_train_loss)
            train_perplexities.append(avg_train_perplexity)
            train_accuracies.append(avg_train_accuracy)

            # ==================== éªŒè¯é˜¶æ®µ ====================
            # åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Œä¸æ›´æ–°å‚æ•°
            avg_val_loss, avg_val_perplexity, avg_val_accuracy, rouge = self.validate_model(
                model, self.val_loader, criterion
            )

            # è®°å½•éªŒè¯ç»“æœ
            val_losses.append(avg_val_loss)
            val_perplexities.append(avg_val_perplexity)
            val_accuracies.append(avg_val_accuracy)
            rouge_scores.append(rouge)

            # å­¦ä¹ ç‡è°ƒåº¦ - æ›´æ–°å­¦ä¹ ç‡
            scheduler.step()
            current_lr = optimizer.param_groups[0]['lr']# è·å–å½“å‰å­¦ä¹ ç‡

            # æ‰“å°ç»“æœ
            print(f"è®­ç»ƒæŸå¤±: {avg_train_loss:.4f} | éªŒè¯æŸå¤±: {avg_val_loss:.4f}")
            print(f"è®­ç»ƒå›°æƒ‘åº¦: {avg_train_perplexity:.2f} | éªŒè¯å›°æƒ‘åº¦: {avg_val_perplexity:.2f}")
            print(f"è®­ç»ƒå‡†ç¡®ç‡: {avg_train_accuracy:.4f} | éªŒè¯å‡†ç¡®ç‡: {avg_val_accuracy:.4f}")
            print(f"å­¦ä¹ ç‡: {current_lr:.2e}")
            print(f"ROUGE-1: {rouge['rouge1']:.4f} | ROUGE-2: {rouge['rouge2']:.4f} | ROUGE-L: {rouge['rougeL']:.4f}")

            # åŸºäºéªŒè¯å‡†ç¡®ç‡ä¿å­˜æœ€ä½³æ¨¡å‹
            if avg_val_accuracy > best_accuracy:
                best_accuracy = avg_val_accuracy
                best_epoch = epoch + 1
                best_model_state = {
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'epoch': epoch,
                    'train_loss': avg_train_loss,
                    'val_loss': avg_val_loss,
                    'train_perplexity': avg_train_perplexity,
                    'val_perplexity': avg_val_perplexity,
                    'train_accuracy': avg_train_accuracy,
                    'val_accuracy': avg_val_accuracy,
                    'rouge_scores': rouge
                }
                print(f"âœ… æ–°çš„æœ€ä½³æ¨¡å‹ (Epoch {best_epoch}, éªŒè¯å‡†ç¡®ç‡: {best_accuracy:.4f})")

        # è¿”å›å®Œæ•´çš„è®­ç»ƒç»“æœ
        return {
            'model': model,
            'train_losses': train_losses,
            'val_losses': val_losses,
            'train_perplexities': train_perplexities,
            'val_perplexities': val_perplexities,
            'train_accuracies': train_accuracies,
            'val_accuracies': val_accuracies,
            'rouge_scores': rouge_scores,
            'best_accuracy': best_accuracy,
            'best_epoch': best_epoch,
            'best_model_state': best_model_state
        }

    def validate_model(self, model, val_loader, criterion):
        """
        éªŒè¯æ¨¡å‹æ€§èƒ½ï¼š
        åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡å‹è¡¨ç°ï¼Œä¸æ›´æ–°æ¨¡å‹å‚æ•°ï¼Œåªè®¡ç®—è¯„ä¼°æŒ‡æ ‡ã€‚
        è¿™ä¸ªæ–¹æ³•ç”¨äºåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ç›‘æ§æ¨¡å‹æ€§èƒ½ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆã€‚
        Args:
            model: è¦è¯„ä¼°çš„æ¨¡å‹å®ä¾‹
                - ç±»å‹: AblationTransformer
                - çŠ¶æ€: å·²è®­ç»ƒæˆ–æ­£åœ¨è®­ç»ƒä¸­çš„æ¨¡å‹
            val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
                - åŒ…å«éªŒè¯æ•°æ®é›†çš„æ‰€æœ‰æ‰¹æ¬¡
                - æ‰¹æ¬¡æ ¼å¼: {'input_ids': ..., 'attention_mask': ..., 'labels': ...}
            criterion: æŸå¤±å‡½æ•°
                - ç±»å‹: nn.CrossEntropyLoss
                - é…ç½®: ignore_index=self.tokenizer.pad_token_id
        Returns:
            tuple: (avg_loss, avg_perplexity, avg_accuracy, rouge)
                - avg_loss: å¹³å‡éªŒè¯æŸå¤±ï¼ˆæ ‡é‡ï¼‰
                - avg_perplexity: å¹³å‡éªŒè¯å›°æƒ‘åº¦ï¼ˆæ ‡é‡ï¼‰
                - avg_accuracy: å¹³å‡éªŒè¯å‡†ç¡®ç‡ï¼ˆæ ‡é‡ï¼‰
                - rouge: ROUGEè¯„ä¼°åˆ†æ•°ï¼ˆå­—å…¸ï¼‰
                    - åŒ…å«rouge1, rouge2, rougeLä¸‰ä¸ªåˆ†æ•°
        """
        model.eval()
        # åˆå§‹åŒ–ç´¯è®¡å˜é‡
        total_loss = 0  # ç´¯è®¡éªŒè¯æŸå¤±
        total_perplexity = 0  # ç´¯è®¡éªŒè¯å›°æƒ‘åº¦
        total_accuracy = 0  # ç´¯è®¡éªŒè¯å‡†ç¡®ç‡
        # ç”¨äºROUGEè¯„ä¼°çš„é¢„æµ‹å’Œå‚è€ƒæ–‡æœ¬
        all_predictions = []  # å­˜å‚¨æ¨¡å‹ç”Ÿæˆçš„æ‰€æœ‰æ‘˜è¦æ–‡æœ¬
        all_references = []  # å­˜å‚¨æ‰€æœ‰çœŸå®çš„æ‘˜è¦æ–‡æœ¬ï¼ˆå‚è€ƒæ‘˜è¦ï¼‰

        with torch.no_grad():# ç¦ç”¨æ¢¯åº¦è®¡ç®—ï¼ŒèŠ‚çœå†…å­˜å¹¶åŠ é€Ÿæ¨ç†è¿‡ç¨‹
            # ä½¿ç”¨tqdmè¿›åº¦æ¡éå†éªŒè¯é›†çš„æ‰€æœ‰æ‰¹æ¬¡
            for batch in tqdm(val_loader, desc='éªŒè¯'):
                # å‡†å¤‡æ•°æ® - å°†æ•°æ®ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
                input_ids = batch['input_ids'].to(self.device)  # è¾“å…¥åºåˆ—token IDs
                labels = batch['labels'].to(self.device)  # ç›®æ ‡åºåˆ—token IDs

                # å‰å‘ä¼ æ’­
                outputs = model(input_ids, labels[:, :-1])
                logits = outputs['logits']

                # è®¡ç®—æŸå¤±
                loss = criterion(
                    logits.reshape(-1, logits.size(-1)),
                    labels[:, 1:].reshape(-1)
                )

                # è®¡ç®—å‡†ç¡®ç‡
                accuracy = self.calculate_accuracy(
                    logits,
                    labels[:, 1:],
                    ignore_index=self.tokenizer.pad_token_id
                )

                total_loss += loss.item()
                total_perplexity += self.calculate_perplexity(loss.item())
                total_accuracy += accuracy

                # ç”Ÿæˆé¢„æµ‹ - ç”¨äºROUGEè¯„ä¼°
                # ä½¿ç”¨æ¨¡å‹ç”Ÿæˆå®Œæ•´çš„æ‘˜è¦ï¼Œè€Œä¸æ˜¯Teacher Forcing
                try:
                    # åˆ›å»ºæºåºåˆ—æ©ç  - æ ‡è¯†å“ªäº›ä½ç½®æ˜¯çœŸå®å†…å®¹
                    src_mask = model._create_src_mask(input_ids)
                    # ä½¿ç”¨æ¨¡å‹ç”Ÿæˆæ‘˜è¦ï¼ˆæ¨ç†æ¨¡å¼ï¼‰
                    # generateæ–¹æ³•ä½¿ç”¨è‡ªå›å½’æ–¹å¼ç”Ÿæˆå®Œæ•´åºåˆ—
                    predictions = model.generate(
                        input_ids,  # è¾“å…¥åºåˆ—
                        src_mask=src_mask,  # æºåºåˆ—æ©ç 
                        max_length=self.config.max_target_length  # æœ€å¤§ç”Ÿæˆé•¿åº¦
                    )

                    # è§£ç é¢„æµ‹ç»“æœ - å°†token IDè½¬æ¢ä¸ºæ–‡æœ¬
                    decoded_preds = self.tokenizer.batch_decode(
                        predictions.cpu(),  # å°†å¼ é‡ç§»åŠ¨åˆ°CPU
                        skip_special_tokens=True  # è·³è¿‡ç‰¹æ®Štokenï¼ˆå¦‚[CLS], [SEP], [PAD]ï¼‰
                    )

                    # è§£ç çœŸå®æ ‡ç­¾
                    decoded_labels = self.tokenizer.batch_decode(
                        labels.cpu(),  # å°†å¼ é‡ç§»åŠ¨åˆ°CPU
                        skip_special_tokens=True  # è·³è¿‡ç‰¹æ®Štoken
                    )

                    # å­˜å‚¨ç»“æœç”¨äºåç»­è¯„ä¼°
                    all_predictions.extend(decoded_preds)  # æ·»åŠ ç”Ÿæˆçš„æ‘˜è¦
                    all_references.extend(decoded_labels)  # æ·»åŠ çœŸå®çš„æ‘˜è¦
                except Exception as e:
                    print(f"ç”Ÿæˆé¢„æµ‹æ—¶å‡ºé”™: {e}")
                    continue

        # è®¡ç®—å¹³å‡æŒ‡æ ‡ - æ•´ä¸ªéªŒè¯é›†çš„å¹³å‡å€¼
        avg_loss = total_loss / len(val_loader)  # å¹³å‡æŸå¤±
        avg_perplexity = total_perplexity / len(val_loader)  # å¹³å‡å›°æƒ‘åº¦
        avg_accuracy = total_accuracy / len(val_loader)  # å¹³å‡å‡†ç¡®ç‡

        # è®¡ç®—ROUGEåˆ†æ•° - è¯„ä¼°ç”Ÿæˆæ‘˜è¦çš„è´¨é‡
        if len(all_predictions) > 0 and len(all_references) > 0:
            # æˆåŠŸç”Ÿæˆé¢„æµ‹ï¼Œè®¡ç®—ROUGEåˆ†æ•°
            rouge = self.calculate_rouge(all_predictions, all_references)
        else:
            # æ— æ³•ç”Ÿæˆæœ‰æ•ˆé¢„æµ‹ï¼Œä½¿ç”¨é»˜è®¤åˆ†æ•°
            rouge = {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0}
            print("è­¦å‘Šï¼šæ— æ³•è®¡ç®—ROUGEåˆ†æ•°ï¼Œé¢„æµ‹ç»“æœä¸ºç©º")

        return avg_loss, avg_perplexity, avg_accuracy, rouge

    def calculate_rouge(self, predictions, references):
        """
        è®¡ç®—ROUGEè¯„ä¼°åˆ†æ•° - ç”¨äºè¯„ä¼°ç”Ÿæˆæ‘˜è¦çš„è´¨é‡
        Args:
            predictions (list[str]): æ¨¡å‹ç”Ÿæˆçš„æ‘˜è¦æ–‡æœ¬åˆ—è¡¨
                - æ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œä»£è¡¨æ¨¡å‹å¯¹å•ä¸ªæ ·æœ¬çš„æ‘˜è¦ç”Ÿæˆç»“æœ
                - ç¤ºä¾‹: ["çŒ«åœ¨å«å­ä¸Šç¡è§‰", "ç‹—åœ¨é™¢å­é‡Œç©è€"]
                - è¦æ±‚: å¿…é¡»ä¸referencesåˆ—è¡¨é¡ºåºä¸€è‡´ä¸”é•¿åº¦ç›¸åŒ
            references (list[str]): äººå·¥æ’°å†™çš„å‚è€ƒæ‘˜è¦æ–‡æœ¬åˆ—è¡¨
                - æ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œä»£è¡¨è¯¥æ ·æœ¬çš„æ ‡å‡†ç­”æ¡ˆï¼ˆé»„é‡‘æ‘˜è¦ï¼‰
                - ç¤ºä¾‹: ["ä¸€åªçŒ«åœ¨å«å­ä¸Šä¼‘æ¯", "ç‹—åœ¨èŠ±å›­é‡Œç©"]
                - è¦æ±‚: å¿…é¡»ä¸predictionsåˆ—è¡¨é¡ºåºä¸€è‡´ä¸”é•¿åº¦ç›¸åŒ
        Returns:
            dict: åŒ…å«ä¸‰ç§ROUGEåˆ†æ•°çš„å­—å…¸
                - 'rouge1': ROUGE-1åˆ†æ•°ï¼ˆåŸºäºå•ä¸ªå•è¯ï¼‰
                - 'rouge2': ROUGE-2åˆ†æ•°ï¼ˆåŸºäºå•è¯å¯¹ï¼‰
                - 'rougeL': ROUGE-Låˆ†æ•°ï¼ˆåŸºäºæœ€é•¿å…¬å…±å­åºåˆ—ï¼‰
                - æ¯ä¸ªåˆ†æ•°éƒ½æ˜¯è¯¥æ‰¹æ¬¡æ‰€æœ‰æ ·æœ¬çš„å¹³å‡å€¼ï¼ŒèŒƒå›´[0.0, 1.0]
        Note:
            ROUGEåˆ†æ•°è¶Šé«˜è¡¨ç¤ºç”Ÿæˆæ‘˜è¦ä¸å‚è€ƒæ‘˜è¦è¶Šç›¸ä¼¼ï¼Œè´¨é‡è¶Šå¥½
            é€šå¸¸ROUGE-1 > ROUGE-L > ROUGE-2ï¼Œå› ä¸ºåŒ¹é…è¦æ±‚é€æ¸ä¸¥æ ¼
        """
        # åˆå§‹åŒ–ROUGEè¯„åˆ†å™¨
        # RougeScoreræ˜¯rouge_scoreåº“çš„æ ¸å¿ƒç±»ï¼Œè´Ÿè´£è®¡ç®—å„ç§ROUGEæŒ‡æ ‡
        # å‚æ•°è¯´æ˜ï¼š
        # - ['rouge1', 'rouge2', 'rougeL']: åŒæ—¶è®¡ç®—ä¸‰ç§ROUGEåˆ†æ•°
        #   * rouge1: åŸºäºunigramï¼ˆå•ä¸ªå•è¯ï¼‰çš„é‡å åº¦
        #   * rouge2: åŸºäºbigramï¼ˆè¿ç»­ä¸¤ä¸ªå•è¯ï¼‰çš„é‡å åº¦
        #   * rougeL: åŸºäºæœ€é•¿å…¬å…±å­åºåˆ—ï¼ˆLCSï¼‰çš„ç›¸ä¼¼åº¦
        # - use_stemmer=True: å¯¹å•è¯è¿›è¡Œè¯å¹²æå–ï¼ˆå¦‚"running"â†’"run"ï¼‰
        #   å‡å°‘è¯å½¢å˜åŒ–å¯¹åŒ¹é…çš„å½±å“ï¼Œæé«˜è¯„ä¼°çš„è¯­ä¹‰å‡†ç¡®æ€§
        scorer = rouge_scorer.RougeScorer(
            ['rouge1', 'rouge2', 'rougeL'],  # æŒ‡å®šè¦è®¡ç®—çš„ROUGEç±»å‹
            use_stemmer=True  # å¯ç”¨è¯å¹²æå–ï¼Œæé«˜åŒ¹é…å‡†ç¡®æ€§
        )

        # åˆå§‹åŒ–åˆ†æ•°å­˜å‚¨åˆ—è¡¨
        rouge1_scores = []  # å­˜å‚¨æ¯ä¸ªæ ·æœ¬çš„ROUGE-1åˆ†æ•°
        rouge2_scores = []  # å­˜å‚¨æ¯ä¸ªæ ·æœ¬çš„ROUGE-2åˆ†æ•°
        rougeL_scores = []  # å­˜å‚¨æ¯ä¸ªæ ·æœ¬çš„ROUGE-Låˆ†æ•°

        # éå†æ¯ä¸ªæ ·æœ¬å¯¹ï¼ˆç”Ÿæˆæ‘˜è¦ vs å‚è€ƒæ‘˜è¦ï¼‰
        for pred, ref in zip(predictions, references):
            # è®¡ç®—å½“å‰æ ·æœ¬çš„ROUGEåˆ†æ•°
            # scorer.score() è¿”å›åŒ…å«precision, recall, fmeasureçš„å­—å…¸
            scores = scorer.score(ref, pred)
            # scorer.score()è¿”å›çš„ä¸‰ç§æŒ‡æ ‡ï¼š
            # scores = {
            #     'rouge1': {
            #         'precision': 0.75,   # ç²¾ç¡®ç‡ï¼šç”Ÿæˆæ‘˜è¦ä¸­æœ‰å¤šå°‘æ˜¯æ­£ç¡®çš„
            #         'recall': 0.60,      # å¬å›ç‡ï¼šå‚è€ƒæ‘˜è¦ä¸­æœ‰å¤šå°‘è¢«è¦†ç›–
            #         'fmeasure': 0.67     # F1åˆ†æ•°ï¼šç²¾ç¡®ç‡å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡
            #     },
            #     'rouge2': {
            #         'precision': 0.50,
            #         'recall': 0.40,
            #         'fmeasure': 0.44
            #     },
            #     'rougeL': {
            #         'precision': 0.70,
            #         'recall': 0.55,
            #         'fmeasure': 0.62
            #     }
            # }

            # æå–å¹¶å­˜å‚¨F1åˆ†æ•°ï¼ˆç²¾ç¡®ç‡å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡ï¼‰
            # F1åˆ†æ•°ç»¼åˆè€ƒè™‘äº†ç”Ÿæˆæ‘˜è¦çš„å‡†ç¡®æ€§å’Œå®Œæ•´æ€§
            rouge1_scores.append(scores['rouge1'].fmeasure)  # å•è¯çº§åˆ«ç›¸ä¼¼åº¦
            rouge2_scores.append(scores['rouge2'].fmeasure)  # çŸ­è¯­çº§åˆ«ç›¸ä¼¼åº¦
            rougeL_scores.append(scores['rougeL'].fmeasure)  # å¥å­ç»“æ„ç›¸ä¼¼åº¦

        # è®¡ç®—æ‰¹æ¬¡å¹³å‡åˆ†æ•°å¹¶è¿”å›
        return {
            'rouge1': np.mean(rouge1_scores),  # å¹³å‡ROUGE-1åˆ†æ•°
            'rouge2': np.mean(rouge2_scores),  # å¹³å‡ROUGE-2åˆ†æ•°
            'rougeL': np.mean(rougeL_scores)  # å¹³å‡ROUGE-Låˆ†æ•°
        }

    def run_study(self):
        """
        è¿è¡Œå®Œæ•´çš„æ¶ˆèå®éªŒ
        è¿™ä¸ªæ–¹æ³•æ‰§è¡Œæ•´ä¸ªä½ç½®ç¼–ç æ¶ˆèå®éªŒï¼ŒåŒ…æ‹¬ï¼š
        1. è®­ç»ƒåŸºçº¿æ¨¡å‹ï¼ˆä½¿ç”¨ä½ç½®ç¼–ç ï¼‰
        2. è®­ç»ƒæ¶ˆèæ¨¡å‹ï¼ˆä¸ä½¿ç”¨ä½ç½®ç¼–ç ï¼‰
        3. ä¿å­˜æ‰€æœ‰å®éªŒç»“æœ
        4. ç”Ÿæˆå¯è§†åŒ–æ¯”è¾ƒå›¾è¡¨
        Returns:
            dict: å®Œæ•´çš„å®éªŒç»“æœå­—å…¸
                - åŒ…å«ä¸¤ä¸ªå®éªŒçš„æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡å’Œå†å²è®°å½•
                - ç»“æ„: {
                    "with_positional_encoding": {...},    # åŸºçº¿æ¨¡å‹ç»“æœ
                    "without_positional_encoding": {...}  # æ¶ˆèæ¨¡å‹ç»“æœ
                }
        """
        print("å¼€å§‹ä½ç½®ç¼–ç æ¶ˆèå®éªŒ")
        print("=" * 60)

        # å®éªŒ1: ä½¿ç”¨ä½ç½®ç¼–ç ï¼ˆåŸºçº¿æ¨¡å‹ï¼‰
        print("\nğŸ”¬ å®éªŒ1: è®­ç»ƒå¸¦æœ‰ä½ç½®ç¼–ç çš„æ¨¡å‹ï¼ˆåŸºçº¿ï¼‰")
        # è®­ç»ƒåŸºçº¿æ¨¡å‹
        baseline_results = self.train_model(
            use_positional_encoding=True,  # ä½¿ç”¨ä½ç½®ç¼–ç 
            model_name="with_positional_encoding"  # æ¨¡å‹åç§°æ ‡è¯†
        )
        # å­˜å‚¨åŸºçº¿æ¨¡å‹ç»“æœ - å®Œæ•´è®°å½•æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡
        self.results["with_positional_encoding"] = {
            "final_train_loss": baseline_results['train_losses'][-1],
            "final_val_loss": baseline_results['val_losses'][-1],
            "final_train_perplexity": baseline_results['train_perplexities'][-1],
            "final_val_perplexity": baseline_results['val_perplexities'][-1],
            "final_train_accuracy": baseline_results['train_accuracies'][-1],
            "final_val_accuracy": baseline_results['val_accuracies'][-1],
            "final_rouge": baseline_results['rouge_scores'][-1],
            "best_accuracy": baseline_results['best_accuracy'],
            "best_epoch": baseline_results['best_epoch'],
            "all_rouge_scores": baseline_results['rouge_scores'],
            "train_losses": baseline_results['train_losses'],
            "val_losses": baseline_results['val_losses'],
            "train_perplexities": baseline_results['train_perplexities'],
            "val_perplexities": baseline_results['val_perplexities'],
            "train_accuracies": baseline_results['train_accuracies'],
            "val_accuracies": baseline_results['val_accuracies']
        }

        # å®éªŒ2: ä¸ä½¿ç”¨ä½ç½®ç¼–ç ï¼ˆæ¶ˆèæ¨¡å‹ï¼‰
        print("\nğŸ”¬ å®éªŒ2: è®­ç»ƒä¸å¸¦æœ‰ä½ç½®ç¼–ç çš„æ¨¡å‹ï¼ˆæ¶ˆèï¼‰")
        # è®­ç»ƒæ¶ˆèæ¨¡å‹
        ablation_results = self.train_model(
            use_positional_encoding=False,  # ä¸ä½¿ç”¨ä½ç½®ç¼–ç 
            model_name="without_positional_encoding"  # æ¨¡å‹åç§°æ ‡è¯†
        )
        # å­˜å‚¨æ¶ˆèæ¨¡å‹ç»“æœ - ç»“æ„åŒä¸Š
        self.results["without_positional_encoding"] = {
            "final_train_loss": ablation_results['train_losses'][-1],
            "final_val_loss": ablation_results['val_losses'][-1],
            "final_train_perplexity": ablation_results['train_perplexities'][-1],
            "final_val_perplexity": ablation_results['val_perplexities'][-1],
            "final_train_accuracy": ablation_results['train_accuracies'][-1],
            "final_val_accuracy": ablation_results['val_accuracies'][-1],
            "final_rouge": ablation_results['rouge_scores'][-1],
            "best_accuracy": ablation_results['best_accuracy'],
            "best_epoch": ablation_results['best_epoch'],
            "all_rouge_scores": ablation_results['rouge_scores'],
            "train_losses": ablation_results['train_losses'],
            "val_losses": ablation_results['val_losses'],
            "train_perplexities": ablation_results['train_perplexities'],
            "val_perplexities": ablation_results['val_perplexities'],
            "train_accuracies": ablation_results['train_accuracies'],
            "val_accuracies": ablation_results['val_accuracies']
        }

        # ä¿å­˜ç»“æœ
        self.save_results()

        # å¯è§†åŒ–æ¯”è¾ƒ
        self.visualize_comparison()

        return self.results

    def save_results(self):
        """
        ä¿å­˜å®éªŒç»“æœåˆ°æ–‡ä»¶
        å°†å®Œæ•´çš„å®éªŒç»“æœä¿å­˜ä¸ºä¸¤ç§æ ¼å¼ï¼š
        1. JSONæ ¼å¼ï¼šå®Œæ•´çš„æ•°æ®ç»“æ„ï¼Œä¾¿äºç¨‹åºè¯»å–å’Œåˆ†æ
        2. æ–‡æœ¬æ ¼å¼ï¼šäººç±»å¯è¯»çš„æ‘˜è¦æŠ¥å‘Šï¼Œä¾¿äºå¿«é€ŸæŸ¥çœ‹ç»“æœ
        æ–‡ä»¶å‘½åçº¦å®šï¼š
            - ablation_results_{epochs}epochs_{heads}heads.json
            - experiment_summary_{epochs}epochs_{heads}heads.txt
        å…¶ä¸­{epochs}å’Œ{heads}æ˜¯é…ç½®å‚æ•°ï¼Œä¾¿äºåŒºåˆ†ä¸åŒå®éªŒè®¾ç½®çš„ç»“æœ
        """
        # ä¿å­˜JSONç»“æœ
        results_file = os.path.join(self.results_dir,f"ablation_results_{self.config.num_epochs}epochs_{self.config.n_heads}heads.json")
        with open(results_file, 'w', encoding='utf-8') as f:
            # indent=2: ç¾åŒ–æ ¼å¼ï¼Œä¾¿äºé˜…è¯»
            # ensure_ascii=False: æ”¯æŒä¸­æ–‡å­—ç¬¦
            json.dump(self.results, f, indent=2, ensure_ascii=False)

        # ä¿å­˜æ–‡æœ¬æ‘˜è¦
        summary_file = os.path.join(self.results_dir,f"experiment_summary_{self.config.num_epochs}epochs_{self.config.n_heads}heads.txt")
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write("ä½ç½®ç¼–ç æ¶ˆèå®éªŒæ‘˜è¦\n")
            f.write("=" * 50 + "\n\n")

            baseline = self.results["with_positional_encoding"]
            ablation = self.results["without_positional_encoding"]

            f.write("åŸºçº¿æ¨¡å‹ï¼ˆæœ‰ä½ç½®ç¼–ç ï¼‰:\n")
            f.write(f"  æœ€ç»ˆè®­ç»ƒæŸå¤±: {baseline['final_train_loss']:.4f}\n")
            f.write(f"  æœ€ç»ˆéªŒè¯æŸå¤±: {baseline['final_val_loss']:.4f}\n")
            f.write(f"  æœ€ç»ˆè®­ç»ƒå›°æƒ‘åº¦: {baseline['final_train_perplexity']:.2f}\n")
            f.write(f"  æœ€ç»ˆéªŒè¯å›°æƒ‘åº¦: {baseline['final_val_perplexity']:.2f}\n")
            f.write(f"  æœ€ç»ˆè®­ç»ƒå‡†ç¡®ç‡: {baseline['final_train_accuracy']:.4f}\n")
            f.write(f"  æœ€ç»ˆéªŒè¯å‡†ç¡®ç‡: {baseline['final_val_accuracy']:.4f}\n")
            f.write(f"  æœ€ç»ˆROUGE-1: {baseline['final_rouge']['rouge1']:.4f}\n")
            f.write(f"  æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {baseline['best_accuracy']:.4f} (Epoch {baseline['best_epoch']})\n\n")

            f.write("æ¶ˆèæ¨¡å‹ï¼ˆæ— ä½ç½®ç¼–ç ï¼‰:\n")
            f.write(f"  æœ€ç»ˆè®­ç»ƒæŸå¤±: {ablation['final_train_loss']:.4f}\n")
            f.write(f"  æœ€ç»ˆéªŒè¯æŸå¤±: {ablation['final_val_loss']:.4f}\n")
            f.write(f"  æœ€ç»ˆè®­ç»ƒå›°æƒ‘åº¦: {ablation['final_train_perplexity']:.2f}\n")
            f.write(f"  æœ€ç»ˆéªŒè¯å›°æƒ‘åº¦: {ablation['final_val_perplexity']:.2f}\n")
            f.write(f"  æœ€ç»ˆè®­ç»ƒå‡†ç¡®ç‡: {ablation['final_train_accuracy']:.4f}\n")
            f.write(f"  æœ€ç»ˆéªŒè¯å‡†ç¡®ç‡: {ablation['final_val_accuracy']:.4f}\n")
            f.write(f"  æœ€ç»ˆROUGE-1: {ablation['final_rouge']['rouge1']:.4f}\n")
            f.write(f"  æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {ablation['best_accuracy']:.4f} (Epoch {ablation['best_epoch']})\n\n")

            # è®¡ç®—å·®å¼‚
            accuracy_diff = baseline['final_val_accuracy'] - ablation['final_val_accuracy']
            rouge1_diff = baseline['final_rouge']['rouge1'] - ablation['final_rouge']['rouge1']
            rouge2_diff = baseline['final_rouge']['rouge2'] - ablation['final_rouge']['rouge2']
            rougeL_diff = baseline['final_rouge']['rougeL'] - ablation['final_rouge']['rougeL']

            # å·®å¼‚åˆ†æ
            f.write("æ€§èƒ½å·®å¼‚ï¼ˆåŸºçº¿ - æ¶ˆèï¼‰:\n")
            f.write(f"  éªŒè¯å‡†ç¡®ç‡å·®å¼‚: {accuracy_diff:+.4f} ({accuracy_diff / baseline['final_val_accuracy']:+.1%})\n")
            f.write(f"  ROUGE-1å·®å¼‚: {rouge1_diff:+.4f} ({rouge1_diff / baseline['final_rouge']['rouge1']:+.1%})\n")
            f.write(f"  ROUGE-2å·®å¼‚: {rouge2_diff:+.4f} ({rouge2_diff / baseline['final_rouge']['rouge2']:+.1%})\n")
            f.write(f"  ROUGE-Lå·®å¼‚: {rougeL_diff:+.4f} ({rougeL_diff / baseline['final_rouge']['rougeL']:+.1%})\n")

        print(f"âœ… å®éªŒç»“æœå·²ä¿å­˜åˆ°: {results_file}")
        print(f"âœ… å®éªŒæ‘˜è¦å·²ä¿å­˜åˆ°: {summary_file}")

    def visualize_comparison(self):
        """
        å¯è§†åŒ–æ¯”è¾ƒç»“æœ - ç”Ÿæˆ4ä¸ªå­å›¾çš„ç»¼åˆæ¯”è¾ƒå›¾è¡¨
        å›¾è¡¨åŒ…å«ï¼š
            1. è®­ç»ƒæŸå¤±æ¯”è¾ƒæ›²çº¿
            2. éªŒè¯å‡†ç¡®ç‡æ¯”è¾ƒæ›²çº¿
            3. éªŒè¯å›°æƒ‘åº¦æ¯”è¾ƒæ›²çº¿
            4. æœ€ç»ˆæ€§èƒ½æŸ±çŠ¶å›¾æ¯”è¾ƒ
        ä½¿ç”¨2x2ç½‘æ ¼å¸ƒå±€ï¼Œä¾¿äºå…¨é¢æ¯”è¾ƒä¸¤ä¸ªæ¨¡å‹çš„æ€§èƒ½å·®å¼‚
        """
        # è®¾ç½®ç»˜å›¾é£æ ¼
        # plt.style.use('seaborn-v0_8')  # ä½¿ç”¨seaborné£æ ¼ï¼Œå›¾è¡¨æ›´ç¾è§‚
        # åˆ›å»º2x2çš„å­å›¾ç½‘æ ¼
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        plt.subplots_adjust(hspace=0.4)  # å¢åŠ å­å›¾ä¹‹é—´çš„å‚ç›´é—´è·ï¼ˆé»˜è®¤çº¦0.2ï¼‰

        # æå–ä¸¤ä¸ªæ¨¡å‹çš„ç»“æœæ•°æ®
        baseline = self.results["with_positional_encoding"]
        ablation = self.results["without_positional_encoding"]

        # åˆ›å»ºepochåºåˆ—ï¼ˆä»1å¼€å§‹ï¼‰
        epochs = range(1, len(baseline['train_losses']) + 1)

        # ==================== 1. è®­ç»ƒæŸå¤±æ¯”è¾ƒæ›²çº¿ï¼ˆå·¦ä¸Šï¼‰ ====================
        axes[0, 0].plot(epochs, baseline['train_losses'], 'b-', label='æœ‰ä½ç½®ç¼–ç ', linewidth=2)
        axes[0, 0].plot(epochs, ablation['train_losses'], 'r-', label='æ— ä½ç½®ç¼–ç ', linewidth=2)
        axes[0, 0].set_xlabel('Epoch')
        axes[0, 0].set_ylabel('è®­ç»ƒæŸå¤±')
        axes[0, 0].set_title('è®­ç»ƒæŸå¤±æ¯”è¾ƒ', pad=10)
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)  # æ·»åŠ ç½‘æ ¼çº¿ï¼Œalphaæ§åˆ¶é€æ˜åº¦

        # ==================== 2. éªŒè¯å‡†ç¡®ç‡æ¯”è¾ƒæ›²çº¿ï¼ˆå³ä¸Šï¼‰ ====================
        axes[0, 1].plot(epochs, baseline['val_accuracies'], 'b-', label='æœ‰ä½ç½®ç¼–ç ', linewidth=2)
        axes[0, 1].plot(epochs, ablation['val_accuracies'], 'r-', label='æ— ä½ç½®ç¼–ç ', linewidth=2)
        axes[0, 1].set_xlabel('Epoch')
        axes[0, 1].set_ylabel('éªŒè¯å‡†ç¡®ç‡')
        axes[0, 1].set_title('éªŒè¯å‡†ç¡®ç‡æ¯”è¾ƒ', pad=10)
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)

        # ==================== 3. éªŒè¯å›°æƒ‘åº¦æ¯”è¾ƒæ›²çº¿ï¼ˆå·¦ä¸‹ï¼‰ ====================
        axes[1, 0].plot(epochs, baseline['val_perplexities'], 'b-', label='æœ‰ä½ç½®ç¼–ç ', linewidth=2)
        axes[1, 0].plot(epochs, ablation['val_perplexities'], 'r-', label='æ— ä½ç½®ç¼–ç ', linewidth=2)
        axes[1, 0].set_xlabel('Epoch')
        axes[1, 0].set_ylabel('éªŒè¯å›°æƒ‘åº¦')
        axes[1, 0].set_title('éªŒè¯å›°æƒ‘åº¦æ¯”è¾ƒ', pad=10)
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)

        # ==================== 4. æœ€ç»ˆæ€§èƒ½æŸ±çŠ¶å›¾æ¯”è¾ƒï¼ˆå³ä¸‹ï¼‰ ====================
        metrics = ['å‡†ç¡®ç‡', 'ROUGE-1', 'ROUGE-2', 'ROUGE-L']
        baseline_scores = [
            baseline['final_val_accuracy'],
            baseline['final_rouge']['rouge1'],
            baseline['final_rouge']['rouge2'],
            baseline['final_rouge']['rougeL']
        ]
        ablation_scores = [
            ablation['final_val_accuracy'],
            ablation['final_rouge']['rouge1'],
            ablation['final_rouge']['rouge2'],
            ablation['final_rouge']['rougeL']
        ]

        # è®¾ç½®æŸ±çŠ¶å›¾ä½ç½®å’Œå®½åº¦
        x = np.arange(len(metrics))  # xè½´ä½ç½®ï¼š[0, 1, 2, 3]
        width = 0.35  # æŸ±çŠ¶å›¾å®½åº¦
        # ç»˜åˆ¶æŸ±çŠ¶å›¾
        axes[1, 1].bar(x - width / 2, baseline_scores, width, label='æœ‰ä½ç½®ç¼–ç ', alpha=0.8)
        axes[1, 1].bar(x + width / 2, ablation_scores, width, label='æ— ä½ç½®ç¼–ç ', alpha=0.8)
        axes[1, 1].set_xlabel('è¯„ä¼°æŒ‡æ ‡')
        axes[1, 1].set_ylabel('åˆ†æ•°')
        axes[1, 1].set_title('æœ€ç»ˆæ€§èƒ½æ¯”è¾ƒ', pad=10)
        axes[1, 1].set_xticks(x)
        axes[1, 1].set_xticklabels(metrics)
        axes[1, 1].legend()
        axes[1, 1].grid(True, alpha=0.3)

        # è°ƒæ•´å­å›¾é—´è·ï¼Œé¿å…é‡å 
        plt.tight_layout()
        # ä¿å­˜å›¾è¡¨ä¸ºé«˜åˆ†è¾¨ç‡PNGæ–‡ä»¶
        plot_file = os.path.join(self.results_dir, f"ablation_comparison_{self.config.num_epochs}epochs_{self.config.n_heads}heads.png")
        plt.savefig(plot_file, dpi=300, bbox_inches='tight')
        # dpi=300: é«˜åˆ†è¾¨ç‡ï¼ˆ300ç‚¹æ¯è‹±å¯¸ï¼‰
        # bbox_inches='tight': è‡ªåŠ¨è°ƒæ•´è¾¹ç•Œï¼Œé¿å…è£å‰ª
        plt.show()

        print(f"âœ… æ¯”è¾ƒå›¾è¡¨å·²ä¿å­˜åˆ°: {plot_file}")


def main():
    """ä¸»å‡½æ•°ï¼šè¿è¡Œæ¶ˆèå®éªŒ"""
    # è®¾ç½®éšæœºç§å­
    set_seed(config.seed)
    print("è®¾ç½®éšæœºç§å­å®Œæˆ")

    # è·å–æ•°æ®
    print("åŠ è½½æ•°æ®...")
    train_loader, val_loader, test_loader, tokenizer = get_data_loaders(config)

    # è¿è¡Œæ¶ˆèå®éªŒ
    study = AblationStudy(config, tokenizer, train_loader, val_loader, test_loader)
    results = study.run_study()

    print("\nğŸ‰ æ¶ˆèå®éªŒå®Œæˆï¼")
    print(f"ç»“æœä¿å­˜åœ¨: {study.results_dir}")


if __name__ == "__main__":
    main()