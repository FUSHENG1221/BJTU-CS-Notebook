import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import json
import os
from datetime import datetime
from rouge_score import rouge_scorer
import math

from model import Transformer
from config import config
from data_loader import get_data_loaders
from utils import set_seed

import warnings
warnings.filterwarnings("ignore")
plt.rcParams["font.sans-serif"] = ["SimHei"]
plt.rcParams["axes.unicode_minus"] = False

class AblationTransformer(Transformer):
    """
    æ¶ˆèå®éªŒä¸“ç”¨Transformeræ¨¡å‹
    æ”¯æŒå¯ç”¨/ç¦ç”¨ä½ç½®ç¼–ç åŠŸèƒ½
    """
    def __init__(self, config, tokenizer=None, use_positional_encoding=True):
        """
        åˆå§‹åŒ–æ¶ˆèå®éªŒæ¨¡å‹
        Args:
            config: æ¨¡å‹é…ç½®
            tokenizer: åˆ†è¯å™¨
            use_positional_encoding: æ˜¯å¦ä½¿ç”¨ä½ç½®ç¼–ç ï¼ˆæ¶ˆèå˜é‡ï¼‰
        """
        super().__init__(config, tokenizer)
        self.use_positional_encoding = use_positional_encoding

        # å¦‚æœç¦ç”¨ä½ç½®ç¼–ç ï¼Œå°†ä½ç½®ç¼–ç å±‚æ›¿æ¢ä¸ºæ’ç­‰æ˜ å°„
        if not use_positional_encoding:
            self.pos_encoding = nn.Identity()  # æ’ç­‰æ˜ å°„ï¼Œä¸æ·»åŠ ä½ç½®ä¿¡æ¯

    def encode(self, src, src_mask=None):
        """é‡å†™ç¼–ç æ–¹æ³•ï¼Œæ”¯æŒä½ç½®ç¼–ç å¼€å…³"""
        if src_mask is None:
            src_mask = self._create_src_mask(src)

        # è¯åµŒå…¥
        x = self.embedding(src)

        # æ¡ä»¶ä½ç½®ç¼–ç 
        if self.use_positional_encoding:
            x = self.pos_encoding(x)

        x = self.dropout(x)

        encoder_self_attentions = []
        for layer in self.encoder_layers:
            x, self_attn = layer(x, src_mask)
            encoder_self_attentions.append(self_attn)

        return x, encoder_self_attentions

    def decode(self, tgt, encoder_output, src_mask=None, tgt_mask=None):
        """é‡å†™è§£ç æ–¹æ³•ï¼Œæ”¯æŒä½ç½®ç¼–ç å¼€å…³"""
        if tgt_mask is None:
            tgt_mask = self._create_tgt_mask(tgt.size(1))

        # è¯åµŒå…¥
        x = self.embedding(tgt)

        # æ¡ä»¶ä½ç½®ç¼–ç 
        if self.use_positional_encoding:
            x = self.pos_encoding(x)

        x = self.dropout(x)

        decoder_self_attentions = []
        decoder_cross_attentions = []
        for layer in self.decoder_layers:
            x, self_attn, cross_attn = layer(x, encoder_output, src_mask, tgt_mask)
            decoder_self_attentions.append(self_attn)
            decoder_cross_attentions.append(cross_attn)

        logits = self.output_projection(x)
        return logits, decoder_self_attentions, decoder_cross_attentions


class AblationStudy:
    """
    ä½ç½®ç¼–ç æ¶ˆèå®éªŒç±»
    æ¯”è¾ƒä½¿ç”¨ä½ç½®ç¼–ç  vs ä¸ä½¿ç”¨ä½ç½®ç¼–ç çš„æ¨¡å‹æ€§èƒ½å·®å¼‚
    """
    def __init__(self, config, tokenizer, train_loader, val_loader, test_loader):
        """
        åˆå§‹åŒ–æ¶ˆèå®éªŒ
        Args:
            config: å®éªŒé…ç½®
            tokenizer: åˆ†è¯å™¨å®ä¾‹
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
            test_loader: æµ‹è¯•æ•°æ®åŠ è½½å™¨
        """
        self.config = config
        self.tokenizer = tokenizer
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.test_loader = test_loader
        self.device = config.device

        # åˆ›å»ºç»“æœç›®å½•
        self.results_dir = "../results/ablation_study"
        os.makedirs(self.results_dir, exist_ok=True)

        # å®éªŒè®°å½•
        self.results = {
            "experiment_info": {
                "timestamp": datetime.now().isoformat(),
                "config": config.__dict__,
                "description": "ä½ç½®ç¼–ç æ¶ˆèå®éªŒï¼šæ¯”è¾ƒä½¿ç”¨ä½ç½®ç¼–ç  vs ä¸ä½¿ç”¨ä½ç½®ç¼–ç çš„æ¨¡å‹æ€§èƒ½"
            },
            "with_positional_encoding": {},
            "without_positional_encoding": {}
        }

    def calculate_perplexity(self, loss):
        """è®¡ç®—å›°æƒ‘åº¦"""
        return math.exp(loss)

    def calculate_accuracy(self, logits, labels, ignore_index=-100):
        """è®¡ç®—å‡†ç¡®ç‡"""
        predictions = torch.argmax(logits, dim=-1)
        valid_mask = (labels != ignore_index)
        correct = (predictions == labels) & valid_mask
        correct_count = correct.sum().item()
        total_valid = valid_mask.sum().item()

        if total_valid == 0:
            return 0.0
        accuracy = correct_count / total_valid
        return accuracy

    def train_model(self, use_positional_encoding=True, model_name="baseline"):
        """
        è®­ç»ƒå•ä¸ªæ¨¡å‹
        Args:
            use_positional_encoding: æ˜¯å¦ä½¿ç”¨ä½ç½®ç¼–ç 
            model_name: åç§°æ ‡è¯†
        Returns:
            dict: è®­ç»ƒç»“æœå’Œæ¨¡å‹çŠ¶æ€
        """
        print(f"\n{'=' * 60}")
        print(f"è®­ç»ƒæ¨¡å‹: {model_name}")
        print(f"ä½¿ç”¨ä½ç½®ç¼–ç : {use_positional_encoding}")
        print(f"{'=' * 60}")

        # è®¾ç½®éšæœºç§å­ç¡®ä¿å¯é‡å¤æ€§
        set_seed(self.config.seed)

        # åˆ›å»ºæ¨¡å‹
        model = AblationTransformer(
            self.config,
            self.tokenizer,
            use_positional_encoding=use_positional_encoding
        ).to(self.device)

        # ä¼˜åŒ–å™¨
        optimizer = optim.AdamW(model.parameters(), lr=self.config.learning_rate)
        scheduler = optim.lr_scheduler.CosineAnnealingLR(
            optimizer, T_max=self.config.num_epochs
        )

        # æŸå¤±å‡½æ•°
        criterion = nn.CrossEntropyLoss(ignore_index=self.tokenizer.pad_token_id)

        # è®­ç»ƒè®°å½•
        train_losses = []
        val_losses = []
        train_perplexities = []
        val_perplexities = []
        train_accuracies = []
        val_accuracies = []
        rouge_scores = []

        # åŸºäºå‡†ç¡®ç‡é€‰æ‹©æœ€ä½³æ¨¡å‹
        best_accuracy = 0.0
        best_model_state = None
        best_epoch = 0

        # è®­ç»ƒå¾ªç¯
        for epoch in range(self.config.num_epochs):
            print(f"\nEpoch {epoch + 1}/{self.config.num_epochs}")

            # è®­ç»ƒé˜¶æ®µ
            model.train()
            total_train_loss = 0
            total_train_perplexity = 0
            total_train_accuracy = 0

            progress_bar = tqdm(self.train_loader, desc=f'è®­ç»ƒ {model_name}')

            for batch in progress_bar:
                # å‡†å¤‡æ•°æ®
                input_ids = batch['input_ids'].to(self.device)
                labels = batch['labels'].to(self.device)

                # å‰å‘ä¼ æ’­
                outputs = model(input_ids, labels[:, :-1])
                logits = outputs['logits']

                # è®¡ç®—æŸå¤±
                loss = criterion(
                    logits.reshape(-1, logits.size(-1)),
                    labels[:, 1:].reshape(-1)
                )

                # è®¡ç®—å‡†ç¡®ç‡
                accuracy = self.calculate_accuracy(
                    logits,
                    labels[:, 1:],
                    ignore_index=self.tokenizer.pad_token_id
                )

                # åå‘ä¼ æ’­
                optimizer.zero_grad()
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), self.config.max_grad_norm)
                optimizer.step()

                total_train_loss += loss.item()
                total_train_perplexity += self.calculate_perplexity(loss.item())
                total_train_accuracy += accuracy

                progress_bar.set_postfix({
                    'loss': f'{loss.item():.4f}',
                    'acc': f'{accuracy:.4f}',
                    'ppl': f'{self.calculate_perplexity(loss.item()):.2f}'
                })

            # è®¡ç®—è®­ç»ƒå¹³å‡å€¼
            avg_train_loss = total_train_loss / len(self.train_loader)
            avg_train_perplexity = total_train_perplexity / len(self.train_loader)
            avg_train_accuracy = total_train_accuracy / len(self.train_loader)

            train_losses.append(avg_train_loss)
            train_perplexities.append(avg_train_perplexity)
            train_accuracies.append(avg_train_accuracy)

            # éªŒè¯é˜¶æ®µ
            avg_val_loss, avg_val_perplexity, avg_val_accuracy, rouge = self.validate_model(
                model, self.val_loader, criterion
            )

            val_losses.append(avg_val_loss)
            val_perplexities.append(avg_val_perplexity)
            val_accuracies.append(avg_val_accuracy)
            rouge_scores.append(rouge)

            # å­¦ä¹ ç‡è°ƒåº¦
            scheduler.step()
            current_lr = optimizer.param_groups[0]['lr']

            # æ‰“å°ç»“æœ
            print(f"è®­ç»ƒæŸå¤±: {avg_train_loss:.4f} | éªŒè¯æŸå¤±: {avg_val_loss:.4f}")
            print(f"è®­ç»ƒå›°æƒ‘åº¦: {avg_train_perplexity:.2f} | éªŒè¯å›°æƒ‘åº¦: {avg_val_perplexity:.2f}")
            print(f"è®­ç»ƒå‡†ç¡®ç‡: {avg_train_accuracy:.4f} | éªŒè¯å‡†ç¡®ç‡: {avg_val_accuracy:.4f}")
            print(f"å­¦ä¹ ç‡: {current_lr:.2e}")
            print(f"ROUGE-1: {rouge['rouge1']:.4f} | ROUGE-2: {rouge['rouge2']:.4f} | ROUGE-L: {rouge['rougeL']:.4f}")

            # åŸºäºéªŒè¯å‡†ç¡®ç‡ä¿å­˜æœ€ä½³æ¨¡å‹
            if avg_val_accuracy > best_accuracy:
                best_accuracy = avg_val_accuracy
                best_epoch = epoch + 1
                best_model_state = {
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'epoch': epoch,
                    'train_loss': avg_train_loss,
                    'val_loss': avg_val_loss,
                    'train_perplexity': avg_train_perplexity,
                    'val_perplexity': avg_val_perplexity,
                    'train_accuracy': avg_train_accuracy,
                    'val_accuracy': avg_val_accuracy,
                    'rouge_scores': rouge
                }
                print(f"âœ… æ–°çš„æœ€ä½³æ¨¡å‹ (Epoch {best_epoch}, éªŒè¯å‡†ç¡®ç‡: {best_accuracy:.4f})")

        return {
            'model': model,
            'train_losses': train_losses,
            'val_losses': val_losses,
            'train_perplexities': train_perplexities,
            'val_perplexities': val_perplexities,
            'train_accuracies': train_accuracies,
            'val_accuracies': val_accuracies,
            'rouge_scores': rouge_scores,
            'best_accuracy': best_accuracy,
            'best_epoch': best_epoch,
            'best_model_state': best_model_state
        }

    def validate_model(self, model, val_loader, criterion):
        """éªŒè¯æ¨¡å‹æ€§èƒ½"""
        model.eval()
        total_loss = 0
        total_perplexity = 0
        total_accuracy = 0
        all_predictions = []
        all_references = []

        with torch.no_grad():
            for batch in tqdm(val_loader, desc='éªŒè¯'):
                input_ids = batch['input_ids'].to(self.device)
                labels = batch['labels'].to(self.device)

                # å‰å‘ä¼ æ’­
                outputs = model(input_ids, labels[:, :-1])
                logits = outputs['logits']

                # è®¡ç®—æŸå¤±
                loss = criterion(
                    logits.reshape(-1, logits.size(-1)),
                    labels[:, 1:].reshape(-1)
                )

                # è®¡ç®—å‡†ç¡®ç‡
                accuracy = self.calculate_accuracy(
                    logits,
                    labels[:, 1:],
                    ignore_index=self.tokenizer.pad_token_id
                )

                total_loss += loss.item()
                total_perplexity += self.calculate_perplexity(loss.item())
                total_accuracy += accuracy

                # ç”Ÿæˆé¢„æµ‹
                try:
                    src_mask = model._create_src_mask(input_ids)
                    predictions = model.generate(
                        input_ids,
                        src_mask=src_mask,
                        max_length=self.config.max_target_length
                    )

                    decoded_preds = self.tokenizer.batch_decode(predictions.cpu(), skip_special_tokens=True)
                    decoded_labels = self.tokenizer.batch_decode(labels.cpu(), skip_special_tokens=True)

                    all_predictions.extend(decoded_preds)
                    all_references.extend(decoded_labels)
                except Exception as e:
                    print(f"ç”Ÿæˆé¢„æµ‹æ—¶å‡ºé”™: {e}")
                    continue

        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        avg_loss = total_loss / len(val_loader)
        avg_perplexity = total_perplexity / len(val_loader)
        avg_accuracy = total_accuracy / len(val_loader)

        # è®¡ç®—ROUGEåˆ†æ•°
        if len(all_predictions) > 0 and len(all_references) > 0:
            rouge = self.calculate_rouge(all_predictions, all_references)
        else:
            rouge = {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0}

        return avg_loss, avg_perplexity, avg_accuracy, rouge

    def calculate_rouge(self, predictions, references):
        """è®¡ç®—ROUGEåˆ†æ•°"""
        scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
        rouge1_scores = []
        rouge2_scores = []
        rougeL_scores = []

        for pred, ref in zip(predictions, references):
            scores = scorer.score(ref, pred)
            rouge1_scores.append(scores['rouge1'].fmeasure)
            rouge2_scores.append(scores['rouge2'].fmeasure)
            rougeL_scores.append(scores['rougeL'].fmeasure)

        return {
            'rouge1': np.mean(rouge1_scores),
            'rouge2': np.mean(rouge2_scores),
            'rougeL': np.mean(rougeL_scores)
        }

    def run_study(self):
        """è¿è¡Œå®Œæ•´çš„æ¶ˆèå®éªŒ"""
        print("å¼€å§‹ä½ç½®ç¼–ç æ¶ˆèå®éªŒ")
        print("=" * 60)

        # å®éªŒ1: ä½¿ç”¨ä½ç½®ç¼–ç ï¼ˆåŸºçº¿æ¨¡å‹ï¼‰
        print("\nğŸ”¬ å®éªŒ1: è®­ç»ƒå¸¦æœ‰ä½ç½®ç¼–ç çš„æ¨¡å‹ï¼ˆåŸºçº¿ï¼‰")
        baseline_results = self.train_model(
            use_positional_encoding=True,
            model_name="with_positional_encoding"
        )
        self.results["with_positional_encoding"] = {
            "final_train_loss": baseline_results['train_losses'][-1],
            "final_val_loss": baseline_results['val_losses'][-1],
            "final_train_perplexity": baseline_results['train_perplexities'][-1],
            "final_val_perplexity": baseline_results['val_perplexities'][-1],
            "final_train_accuracy": baseline_results['train_accuracies'][-1],
            "final_val_accuracy": baseline_results['val_accuracies'][-1],
            "final_rouge": baseline_results['rouge_scores'][-1],
            "best_accuracy": baseline_results['best_accuracy'],
            "best_epoch": baseline_results['best_epoch'],
            "all_rouge_scores": baseline_results['rouge_scores'],
            "train_losses": baseline_results['train_losses'],
            "val_losses": baseline_results['val_losses'],
            "train_perplexities": baseline_results['train_perplexities'],
            "val_perplexities": baseline_results['val_perplexities'],
            "train_accuracies": baseline_results['train_accuracies'],
            "val_accuracies": baseline_results['val_accuracies']
        }

        # å®éªŒ2: ä¸ä½¿ç”¨ä½ç½®ç¼–ç ï¼ˆæ¶ˆèæ¨¡å‹ï¼‰
        print("\nğŸ”¬ å®éªŒ2: è®­ç»ƒä¸å¸¦æœ‰ä½ç½®ç¼–ç çš„æ¨¡å‹ï¼ˆæ¶ˆèï¼‰")
        ablation_results = self.train_model(
            use_positional_encoding=False,
            model_name="without_positional_encoding"
        )
        self.results["without_positional_encoding"] = {
            "final_train_loss": ablation_results['train_losses'][-1],
            "final_val_loss": ablation_results['val_losses'][-1],
            "final_train_perplexity": ablation_results['train_perplexities'][-1],
            "final_val_perplexity": ablation_results['val_perplexities'][-1],
            "final_train_accuracy": ablation_results['train_accuracies'][-1],
            "final_val_accuracy": ablation_results['val_accuracies'][-1],
            "final_rouge": ablation_results['rouge_scores'][-1],
            "best_accuracy": ablation_results['best_accuracy'],
            "best_epoch": ablation_results['best_epoch'],
            "all_rouge_scores": ablation_results['rouge_scores'],
            "train_losses": ablation_results['train_losses'],
            "val_losses": ablation_results['val_losses'],
            "train_perplexities": ablation_results['train_perplexities'],
            "val_perplexities": ablation_results['val_perplexities'],
            "train_accuracies": ablation_results['train_accuracies'],
            "val_accuracies": ablation_results['val_accuracies']
        }

        # ä¿å­˜ç»“æœ
        self.save_results()

        # å¯è§†åŒ–æ¯”è¾ƒ
        self.visualize_comparison()

        return self.results

    def save_results(self):
        """ä¿å­˜å®éªŒç»“æœ"""
        # ä¿å­˜JSONç»“æœ
        results_file = os.path.join(self.results_dir,
                                    f"ablation_results_{self.config.num_epochs}epochs_{self.config.n_heads}heads.json")
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)

        # ä¿å­˜æ–‡æœ¬æ‘˜è¦
        summary_file = os.path.join(self.results_dir,
                                    f"experiment_summary_{self.config.num_epochs}epochs_{self.config.n_heads}heads.txt")
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write("ä½ç½®ç¼–ç æ¶ˆèå®éªŒæ‘˜è¦\n")
            f.write("=" * 50 + "\n\n")

            baseline = self.results["with_positional_encoding"]
            ablation = self.results["without_positional_encoding"]

            f.write("åŸºçº¿æ¨¡å‹ï¼ˆæœ‰ä½ç½®ç¼–ç ï¼‰:\n")
            f.write(f"  æœ€ç»ˆè®­ç»ƒæŸå¤±: {baseline['final_train_loss']:.4f}\n")
            f.write(f"  æœ€ç»ˆéªŒè¯æŸå¤±: {baseline['final_val_loss']:.4f}\n")
            f.write(f"  æœ€ç»ˆè®­ç»ƒå›°æƒ‘åº¦: {baseline['final_train_perplexity']:.2f}\n")
            f.write(f"  æœ€ç»ˆéªŒè¯å›°æƒ‘åº¦: {baseline['final_val_perplexity']:.2f}\n")
            f.write(f"  æœ€ç»ˆè®­ç»ƒå‡†ç¡®ç‡: {baseline['final_train_accuracy']:.4f}\n")
            f.write(f"  æœ€ç»ˆéªŒè¯å‡†ç¡®ç‡: {baseline['final_val_accuracy']:.4f}\n")
            f.write(f"  æœ€ç»ˆROUGE-1: {baseline['final_rouge']['rouge1']:.4f}\n")
            f.write(f"  æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {baseline['best_accuracy']:.4f} (Epoch {baseline['best_epoch']})\n\n")

            f.write("æ¶ˆèæ¨¡å‹ï¼ˆæ— ä½ç½®ç¼–ç ï¼‰:\n")
            f.write(f"  æœ€ç»ˆè®­ç»ƒæŸå¤±: {ablation['final_train_loss']:.4f}\n")
            f.write(f"  æœ€ç»ˆéªŒè¯æŸå¤±: {ablation['final_val_loss']:.4f}\n")
            f.write(f"  æœ€ç»ˆè®­ç»ƒå›°æƒ‘åº¦: {ablation['final_train_perplexity']:.2f}\n")
            f.write(f"  æœ€ç»ˆéªŒè¯å›°æƒ‘åº¦: {ablation['final_val_perplexity']:.2f}\n")
            f.write(f"  æœ€ç»ˆè®­ç»ƒå‡†ç¡®ç‡: {ablation['final_train_accuracy']:.4f}\n")
            f.write(f"  æœ€ç»ˆéªŒè¯å‡†ç¡®ç‡: {ablation['final_val_accuracy']:.4f}\n")
            f.write(f"  æœ€ç»ˆROUGE-1: {ablation['final_rouge']['rouge1']:.4f}\n")
            f.write(f"  æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {ablation['best_accuracy']:.4f} (Epoch {ablation['best_epoch']})\n\n")

            # è®¡ç®—å·®å¼‚
            accuracy_diff = baseline['final_val_accuracy'] - ablation['final_val_accuracy']
            rouge1_diff = baseline['final_rouge']['rouge1'] - ablation['final_rouge']['rouge1']
            rouge2_diff = baseline['final_rouge']['rouge2'] - ablation['final_rouge']['rouge2']
            rougeL_diff = baseline['final_rouge']['rougeL'] - ablation['final_rouge']['rougeL']

            f.write("æ€§èƒ½å·®å¼‚ï¼ˆåŸºçº¿ - æ¶ˆèï¼‰:\n")
            f.write(f"  éªŒè¯å‡†ç¡®ç‡å·®å¼‚: {accuracy_diff:+.4f} ({accuracy_diff / baseline['final_val_accuracy']:+.1%})\n")
            f.write(f"  ROUGE-1å·®å¼‚: {rouge1_diff:+.4f} ({rouge1_diff / baseline['final_rouge']['rouge1']:+.1%})\n")
            f.write(f"  ROUGE-2å·®å¼‚: {rouge2_diff:+.4f} ({rouge2_diff / baseline['final_rouge']['rouge2']:+.1%})\n")
            f.write(f"  ROUGE-Lå·®å¼‚: {rougeL_diff:+.4f} ({rougeL_diff / baseline['final_rouge']['rougeL']:+.1%})\n")

        print(f"âœ… å®éªŒç»“æœå·²ä¿å­˜åˆ°: {results_file}")
        print(f"âœ… å®éªŒæ‘˜è¦å·²ä¿å­˜åˆ°: {summary_file}")

    def visualize_comparison(self):
        """å¯è§†åŒ–æ¯”è¾ƒç»“æœ"""
        plt.style.use('seaborn-v0_8')
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))

        baseline = self.results["with_positional_encoding"]
        ablation = self.results["without_positional_encoding"]

        epochs = range(1, len(baseline['train_losses']) + 1)

        # 1. æŸå¤±æ›²çº¿æ¯”è¾ƒ
        axes[0, 0].plot(epochs, baseline['train_losses'], 'b-', label='æœ‰ä½ç½®ç¼–ç ', linewidth=2)
        axes[0, 0].plot(epochs, ablation['train_losses'], 'r-', label='æ— ä½ç½®ç¼–ç ', linewidth=2)
        axes[0, 0].set_xlabel('Epoch')
        axes[0, 0].set_ylabel('è®­ç»ƒæŸå¤±')
        axes[0, 0].set_title('è®­ç»ƒæŸå¤±æ¯”è¾ƒ')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)

        # 2. å‡†ç¡®ç‡æ›²çº¿æ¯”è¾ƒ
        axes[0, 1].plot(epochs, baseline['val_accuracies'], 'b-', label='æœ‰ä½ç½®ç¼–ç ', linewidth=2)
        axes[0, 1].plot(epochs, ablation['val_accuracies'], 'r-', label='æ— ä½ç½®ç¼–ç ', linewidth=2)
        axes[0, 1].set_xlabel('Epoch')
        axes[0, 1].set_ylabel('éªŒè¯å‡†ç¡®ç‡')
        axes[0, 1].set_title('éªŒè¯å‡†ç¡®ç‡æ¯”è¾ƒ')
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)

        # 3. å›°æƒ‘åº¦æ›²çº¿æ¯”è¾ƒ
        axes[1, 0].plot(epochs, baseline['val_perplexities'], 'b-', label='æœ‰ä½ç½®ç¼–ç ', linewidth=2)
        axes[1, 0].plot(epochs, ablation['val_perplexities'], 'r-', label='æ— ä½ç½®ç¼–ç ', linewidth=2)
        axes[1, 0].set_xlabel('Epoch')
        axes[1, 0].set_ylabel('éªŒè¯å›°æƒ‘åº¦')
        axes[1, 0].set_title('éªŒè¯å›°æƒ‘åº¦æ¯”è¾ƒ')
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)

        # 4. æœ€ç»ˆæ€§èƒ½æŸ±çŠ¶å›¾
        metrics = ['å‡†ç¡®ç‡', 'ROUGE-1', 'ROUGE-2', 'ROUGE-L']
        baseline_scores = [
            baseline['final_val_accuracy'],
            baseline['final_rouge']['rouge1'],
            baseline['final_rouge']['rouge2'],
            baseline['final_rouge']['rougeL']
        ]
        ablation_scores = [
            ablation['final_val_accuracy'],
            ablation['final_rouge']['rouge1'],
            ablation['final_rouge']['rouge2'],
            ablation['final_rouge']['rougeL']
        ]

        x = np.arange(len(metrics))
        width = 0.35
        axes[1, 1].bar(x - width / 2, baseline_scores, width, label='æœ‰ä½ç½®ç¼–ç ', alpha=0.8)
        axes[1, 1].bar(x + width / 2, ablation_scores, width, label='æ— ä½ç½®ç¼–ç ', alpha=0.8)
        axes[1, 1].set_xlabel('è¯„ä¼°æŒ‡æ ‡')
        axes[1, 1].set_ylabel('åˆ†æ•°')
        axes[1, 1].set_title('æœ€ç»ˆæ€§èƒ½æ¯”è¾ƒ')
        axes[1, 1].set_xticks(x)
        axes[1, 1].set_xticklabels(metrics)
        axes[1, 1].legend()
        axes[1, 1].grid(True, alpha=0.3)

        plt.tight_layout()
        plot_file = os.path.join(self.results_dir, f"ablation_comparison_{self.config.num_epochs}epochs_{self.config.n_heads}heads.png")
        plt.savefig(plot_file, dpi=300, bbox_inches='tight')
        plt.show()

        print(f"âœ… æ¯”è¾ƒå›¾è¡¨å·²ä¿å­˜åˆ°: {plot_file}")


def main():
    """ä¸»å‡½æ•°ï¼šè¿è¡Œæ¶ˆèå®éªŒ"""
    # è®¾ç½®éšæœºç§å­
    set_seed(config.seed)
    print("è®¾ç½®éšæœºç§å­å®Œæˆ")

    # è·å–æ•°æ®
    print("åŠ è½½æ•°æ®...")
    train_loader, val_loader, test_loader, tokenizer = get_data_loaders(config)

    # è¿è¡Œæ¶ˆèå®éªŒ
    study = AblationStudy(config, tokenizer, train_loader, val_loader, test_loader)
    results = study.run_study()

    print("\nğŸ‰ æ¶ˆèå®éªŒå®Œæˆï¼")
    print(f"ç»“æœä¿å­˜åœ¨: {study.results_dir}")


if __name__ == "__main__":
    main()