import torch
import numpy as np
import random


def set_seed(seed):
    """
    è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å®éªŒçš„å¯é‡å¤æ€§
    åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œéšæœºç§å­ç”¨äºæ§åˆ¶éšæœºæ•°ç”Ÿæˆå™¨çš„åˆå§‹çŠ¶æ€ï¼Œç¡®ä¿æ¯æ¬¡è¿è¡Œä»£ç æ—¶
    äº§ç”Ÿçš„éšæœºç»“æœæ˜¯ä¸€è‡´çš„ã€‚è¿™å¯¹äºå®éªŒçš„å¯é‡å¤æ€§å’Œè°ƒè¯•éå¸¸é‡è¦ã€‚
    Args:
        seed (int): éšæœºç§å­å€¼ï¼Œé€šå¸¸ä½¿ç”¨42ã€1234ç­‰å¸¸ç”¨å€¼
    """
    torch.manual_seed(seed)  # è®¾ç½®PyTorchçš„CPUéšæœºç§å­
    np.random.seed(seed)  # è®¾ç½®NumPyçš„éšæœºç§å­
    random.seed(seed)  # è®¾ç½®Pythonå†…ç½®randomæ¨¡å—çš„éšæœºç§å­

    if torch.cuda.is_available():
        # å¦‚æœCUDAå¯ç”¨ï¼Œè¿˜éœ€è¦è®¾ç½®GPUçš„éšæœºç§å­
        torch.cuda.manual_seed_all(seed)


def count_parameters(model):
    """
    è®¡ç®—æ¨¡å‹ä¸­å¯è®­ç»ƒå‚æ•°çš„æ•°é‡
    è¿™ä¸ªå‡½æ•°ç”¨äºåˆ†ææ¨¡å‹çš„å¤æ‚åº¦å’Œå¤§å°ï¼Œå¸®åŠ©äº†è§£ï¼š
    - æ¨¡å‹çš„è®¡ç®—éœ€æ±‚
    - å†…å­˜å ç”¨ä¼°è®¡
    - è®­ç»ƒæ—¶é—´é¢„ä¼°
    Args:
        model (torch.nn.Module): PyTorchæ¨¡å‹å®ä¾‹
    Returns:
        int: æ¨¡å‹ä¸­æ‰€æœ‰å¯è®­ç»ƒå‚æ•°çš„æ€»æ•°é‡
    """
    # ä½¿ç”¨ç”Ÿæˆå™¨è¡¨è¾¾å¼éå†æ‰€æœ‰å‚æ•°ï¼Œåªè®¡ç®—requires_grad=Trueçš„å‚æ•°
    # p.numel() è¿”å›å‚æ•°å¼ é‡ä¸­çš„å…ƒç´ æ•°é‡
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def analyze_attention_patterns(model, tokenizer, text):
    """
    åˆ†æTransformeræ¨¡å‹çš„æ³¨æ„åŠ›æ¨¡å¼
    è¿™ä¸ªå‡½æ•°ç”¨äºå¯è§†åŒ–å’Œç†è§£æ¨¡å‹åœ¨å¤„ç†æ–‡æœ¬æ—¶çš„æ³¨æ„åŠ›åˆ†å¸ƒï¼Œå¸®åŠ©ï¼š
    - ç†è§£æ¨¡å‹å…³æ³¨çš„é‡ç‚¹
    - è°ƒè¯•æ³¨æ„åŠ›æœºåˆ¶
    - åˆ†ææ¨¡å‹çš„å¯è§£é‡Šæ€§
    Args:
        model (torch.nn.Module): Transformeræ¨¡å‹å®ä¾‹
        tokenizer: åˆ†è¯å™¨ï¼Œç”¨äºå°†æ–‡æœ¬è½¬æ¢ä¸ºtoken
        text (str): è¦åˆ†æçš„è¾“å…¥æ–‡æœ¬
    Returns:
        list: åŒ…å«æ‰€æœ‰ç¼–ç å™¨å±‚æ³¨æ„åŠ›æƒé‡çš„åˆ—è¡¨
              æ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªå¼ é‡ï¼Œå½¢çŠ¶ä¸º (batch_size, num_heads, seq_len, seq_len)
    """
    # å°†æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    # è¿™ä¼šç¦ç”¨dropoutå’Œbatch normalizationçš„è®­ç»ƒç‰¹å®šè¡Œä¸º
    model.eval()

    # ä½¿ç”¨torch.no_grad()ä¸Šä¸‹æ–‡ç®¡ç†å™¨ç¦ç”¨æ¢¯åº¦è®¡ç®—
    # è¿™å¯ä»¥èŠ‚çœå†…å­˜å¹¶åŠ é€Ÿæ¨ç†è¿‡ç¨‹
    with torch.no_grad():
        # 1. ä½¿ç”¨åˆ†è¯å™¨å°†æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥æ ¼å¼
        # return_tensors='pt' è¡¨ç¤ºè¿”å›PyTorchå¼ é‡
        inputs = tokenizer(text, return_tensors='pt')

        # æå–è¾“å…¥token IDs
        # input_ids å½¢çŠ¶: (batch_size, sequence_length)
        input_ids = inputs['input_ids']

        # 2. å‰å‘ä¼ æ’­è·å–æ¨¡å‹è¾“å‡º
        # è¿™é‡Œä½¿ç”¨è¾“å…¥åŒæ—¶ä½œä¸ºæºåºåˆ—å’Œç›®æ ‡åºåˆ—ï¼ˆè‡ªç¼–ç ä»»åŠ¡ï¼‰
        outputs = model(input_ids, input_ids)

        # 3. æå–æ³¨æ„åŠ›æƒé‡
        # encoder_self_attentions åŒ…å«æ‰€æœ‰ç¼–ç å™¨å±‚çš„è‡ªæ³¨æ„åŠ›æƒé‡
        # æ¯ä¸ªæ³¨æ„åŠ›æƒé‡çš„å½¢çŠ¶: (batch_size, num_heads, seq_len, seq_len)
        attention_weights = outputs['encoder_self_attentions']

        return attention_weights


# def save_model_checkpoint(model, optimizer, epoch, loss, filepath):
#     """
#     ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹:
#     ç”¨äºåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¿å­˜æ¨¡å‹çš„å½“å‰çŠ¶æ€ï¼ŒåŒ…æ‹¬ï¼š
#     - æ¨¡å‹å‚æ•°
#     - ä¼˜åŒ–å™¨çŠ¶æ€
#     - è®­ç»ƒè¿›åº¦ä¿¡æ¯
#     Args:
#         model (torch.nn.Module): è¦ä¿å­˜çš„æ¨¡å‹
#         optimizer (torch.optim.Optimizer): ä¼˜åŒ–å™¨
#         epoch (int): å½“å‰è®­ç»ƒè½®æ•°
#         loss (float): å½“å‰æŸå¤±å€¼
#         filepath (str): ä¿å­˜è·¯å¾„
#     """
#     checkpoint = {
#         'epoch': epoch,
#         'model_state_dict': model.state_dict(),  # æ¨¡å‹å‚æ•°
#         'optimizer_state_dict': optimizer.state_dict(),  # ä¼˜åŒ–å™¨çŠ¶æ€
#         'loss': loss,
#         'model_config': model.config.__dict__ if hasattr(model, 'config') else {}
#     }
#
#     torch.save(checkpoint, filepath)
#     print(f"âœ… æ¨¡å‹æ£€æŸ¥ç‚¹å·²ä¿å­˜: {filepath}")
#
#
# def load_model_checkpoint(model, optimizer, filepath):
#     """
#     åŠ è½½æ¨¡å‹æ£€æŸ¥ç‚¹:
#     ç”¨äºä»ä¿å­˜çš„æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒçŠ¶æ€
#     Args:
#         model (torch.nn.Modimizerule): è¦åŠ è½½å‚æ•°çš„æ¨¡å‹
#         optimizer (torch.optim.Optimizer): è¦åŠ è½½çŠ¶æ€çš„ä¼˜åŒ–å™¨
#         filepath (str): æ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„
#     Returns:
#         tuple: (epoch, loss) - æ¢å¤çš„è®­ç»ƒè½®æ•°å’ŒæŸå¤±å€¼
#     """
#     checkpoint = torch.load(filepath)
#
#     model.load_state_dict(checkpoint['model_state_dict'])
#     optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
#
#     print(f"âœ… æ¨¡å‹æ£€æŸ¥ç‚¹å·²åŠ è½½: {filepath}")
#     print(f"   æ¢å¤è®­ç»ƒè½®æ•°: {checkpoint['epoch']}")
#     print(f"   æ¢å¤æŸå¤±å€¼: {checkpoint['loss']:.4f}")
#
#     return checkpoint['epoch'], checkpoint['loss']
#
#
# def calculate_model_size(model):
#     """
#     è®¡ç®—æ¨¡å‹åœ¨ç£ç›˜ä¸Šçš„å¤§è‡´å¤§å°
#
#     Args:
#         model (torch.nn.Module): PyTorchæ¨¡å‹
#
#     Returns:
#         dict: åŒ…å«ä¸åŒå•ä½ä¸‹æ¨¡å‹å¤§å°çš„å­—å…¸
#     """
#     # è®¡ç®—å‚æ•°æ•°é‡
#     num_params = count_parameters(model)
#
#     # å‡è®¾æ¯ä¸ªå‚æ•°æ˜¯32ä½æµ®ç‚¹æ•°ï¼ˆ4å­—èŠ‚ï¼‰
#     size_bytes = num_params * 4
#
#     # è½¬æ¢ä¸ºä¸åŒå•ä½
#     size_info = {
#         'parameters': num_params,
#         'bytes': size_bytes,
#         'kilobytes': size_bytes / 1024,
#         'megabytes': size_bytes / (1024 * 1024),
#         'gigabytes': size_bytes / (1024 * 1024 * 1024)
#     }
#
#     return size_info
#
#
# def print_model_summary(model, tokenizer=None):
#     """
#     æ‰“å°æ¨¡å‹çš„è¯¦ç»†æ‘˜è¦ä¿¡æ¯
#
#     Args:
#         model (torch.nn.Module): è¦åˆ†æçš„æ¨¡å‹
#         tokenizer: å¯é€‰çš„åˆ†è¯å™¨ï¼ˆç”¨äºæ˜¾ç¤ºè¯æ±‡è¡¨ä¿¡æ¯ï¼‰
#     """
#     print("=" * 60)
#     print("æ¨¡å‹æ‘˜è¦ä¿¡æ¯")
#     print("=" * 60)
#
#     # åŸºæœ¬ä¿¡æ¯
#     num_params = count_parameters(model)
#     size_info = calculate_model_size(model)
#
#     print(f"æ¨¡å‹ç±»å‹: {model.__class__.__name__}")
#     print(f"å¯è®­ç»ƒå‚æ•°æ•°é‡: {num_params:,}")
#     print(f"æ¨¡å‹å¤§å°: {size_info['megabytes']:.2f} MB")
#
#     # åˆ†å±‚ä¿¡æ¯
#     print("\næ¨¡å‹å±‚ä¿¡æ¯:")
#     for name, module in model.named_children():
#         num_layer_params = sum(p.numel() for p in module.parameters() if p.requires_grad)
#         print(f"  {name}: {num_layer_params:,} å‚æ•°")
#
#     # åˆ†è¯å™¨ä¿¡æ¯ï¼ˆå¦‚æœæä¾›ï¼‰
#     if tokenizer is not None:
#         print(f"\nåˆ†è¯å™¨ä¿¡æ¯:")
#         print(f"  ç±»å‹: {tokenizer.__class__.__name__}")
#         print(f"  è¯æ±‡è¡¨å¤§å°: {tokenizer.vocab_size:,}")
#         print(f"  ç‰¹æ®Štoken: {list(tokenizer.special_tokens_map.values())}")
#
#     print("=" * 60)
#
#
# def visualize_attention(attention_weights, tokens, layer_idx=0, head_idx=0):
#     """
#     å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡ï¼ˆç®€åŒ–ç‰ˆï¼‰
#
#     Args:
#         attention_weights (list): æ³¨æ„åŠ›æƒé‡åˆ—è¡¨
#         tokens (list): å¯¹åº”çš„tokenåˆ—è¡¨
#         layer_idx (int): è¦å¯è§†åŒ–çš„å±‚ç´¢å¼•
#         head_idx (int): è¦å¯è§†åŒ–çš„å¤´ç´¢å¼•
#     """
#     import matplotlib.pyplot as plt
#     import seaborn as sns
#
#     # è·å–æŒ‡å®šå±‚å’Œå¤´çš„æ³¨æ„åŠ›æƒé‡
#     # attention_weights[layer_idx] å½¢çŠ¶: (batch_size, num_heads, seq_len, seq_len)
#     attn_matrix = attention_weights[layer_idx][0, head_idx].cpu().numpy()
#
#     plt.figure(figsize=(10, 8))
#     sns.heatmap(attn_matrix,
#                 xticklabels=tokens,
#                 yticklabels=tokens,
#                 cmap='viridis',
#                 annot=True,  # æ˜¾ç¤ºæ•°å€¼
#                 fmt='.2f',
#                 cbar_kws={'label': 'Attention Weight'})
#
#     plt.title(f'Attention Weights - Layer {layer_idx}, Head {head_idx}')
#     plt.xlabel('Key Tokens')
#     plt.ylabel('Query Tokens')
#     plt.xticks(rotation=45)
#     plt.yticks(rotation=0)
#     plt.tight_layout()
#     plt.show()
#
#
# # ä½¿ç”¨ç¤ºä¾‹
# if __name__ == "__main__":
#     # ç¤ºä¾‹ç”¨æ³•
#     print("å·¥å…·å‡½æ•°æ¼”ç¤º:")
#
#     # 1. è®¾ç½®éšæœºç§å­
#     set_seed(42)
#     print("âœ… éšæœºç§å­è®¾ç½®å®Œæˆ")
#
#
#     # 2. åˆ›å»ºä¸€ä¸ªç®€å•çš„æ¨¡å‹è¿›è¡Œæµ‹è¯•
#     class SimpleModel(torch.nn.Module):
#         def __init__(self):
#             super().__init__()
#             self.linear1 = torch.nn.Linear(10, 20)
#             self.linear2 = torch.nn.Linear(20, 5)
#
#         def forward(self, x):
#             return self.linear2(self.linear1(x))
#
#
#     model = SimpleModel()
#
#     # 3. è®¡ç®—å‚æ•°æ•°é‡
#     num_params = count_parameters(model)
#     print(f"âœ… æ¨¡å‹å‚æ•°æ•°é‡: {num_params}")
#
#     # 4. è®¡ç®—æ¨¡å‹å¤§å°
#     size_info = calculate_model_size(model)
#     print(f"âœ… æ¨¡å‹å¤§å°: {size_info['kilobytes']:.2f} KB")
#
#     # 5. æ‰“å°æ¨¡å‹æ‘˜è¦
#     print_model_summary(model)
#
#     print("æ‰€æœ‰å·¥å…·å‡½æ•°æµ‹è¯•å®Œæˆ! ğŸ‰")