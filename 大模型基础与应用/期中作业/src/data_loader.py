import os
import pandas as pd
from datasets import load_dataset, DatasetDict
from transformers import AutoTokenizer
import torch
from torch.utils.data import Dataset, DataLoader
from config import config

class SAMSumDataset(Dataset):
    """
    SAMSumå¯¹è¯æ‘˜è¦æ•°æ®é›†çš„è‡ªå®šä¹‰Datasetç±»
    è¿™ä¸ªç±»è´Ÿè´£å°†åŸå§‹çš„SAMSumæ•°æ®é›†è½¬æ¢ä¸ºPyTorchå¯ä»¥å¤„ç†çš„æ ¼å¼ï¼Œ
    åŒ…æ‹¬æ–‡æœ¬åˆ†è¯ã€å¡«å……ã€æˆªæ–­ç­‰é¢„å¤„ç†æ“ä½œã€‚
    ç»§æ‰¿è‡ªtorch.utils.data.Datasetï¼Œéœ€è¦å®ç°__len__å’Œ__getitem__æ–¹æ³•ã€‚
    """

    def __init__(self, dataset, tokenizer, max_input_length=512, max_target_length=128):
        """
        åˆå§‹åŒ–æ•°æ®é›†
        Args:
            dataset: Hugging Faceæ•°æ®é›†å¯¹è±¡ï¼ŒåŒ…å«'id'ã€'dialogue'å’Œ'summary'å­—æ®µ
            tokenizer: åˆ†è¯å™¨ï¼Œç”¨äºå°†æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹å¯å¤„ç†çš„token IDåºåˆ—
            max_input_length: è¾“å…¥åºåˆ—çš„æœ€å¤§é•¿åº¦ï¼ˆå¯¹è¯æ–‡æœ¬ï¼‰
            max_target_length: ç›®æ ‡åºåˆ—çš„æœ€å¤§é•¿åº¦ï¼ˆæ‘˜è¦æ–‡æœ¬ï¼‰
        """
        self.dataset = dataset  # åŸå§‹æ•°æ®é›†
        self.tokenizer = tokenizer  # åˆ†è¯å™¨å®ä¾‹
        self.max_input_length = max_input_length  # è¾“å…¥æ–‡æœ¬æœ€å¤§é•¿åº¦
        self.max_target_length = max_target_length  # ç›®æ ‡æ–‡æœ¬æœ€å¤§é•¿åº¦

    def __len__(self):
        """
        è¿”å›æ•°æ®é›†çš„æ ·æœ¬æ•°é‡
        è¿™æ˜¯Datasetç±»å¿…é¡»å®ç°çš„æ–¹æ³•ï¼Œå‘Šè¯‰DataLoaderæœ‰å¤šå°‘ä¸ªæ ·æœ¬
        Returns:
            int: æ•°æ®é›†ä¸­æ ·æœ¬çš„æ€»æ•°
        """
        return len(self.dataset)

    def __getitem__(self, idx):
        """
        æ ¹æ®ç´¢å¼•è·å–å•ä¸ªæ ·æœ¬
        è¿™æ˜¯Datasetç±»å¿…é¡»å®ç°çš„æ ¸å¿ƒæ–¹æ³•ï¼Œè´Ÿè´£å°†åŸå§‹æ•°æ®è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥æ ¼å¼
        Args:
            idx (int): æ ·æœ¬ç´¢å¼•
        Returns:
            dict: åŒ…å«ä»¥ä¸‹é”®çš„å­—å…¸ï¼š
                - input_ids: åˆ†è¯åçš„å¯¹è¯æ–‡æœ¬IDåºåˆ—
                - attention_mask: æ³¨æ„åŠ›æ©ç ï¼ŒåŒºåˆ†çœŸå®å†…å®¹å’Œå¡«å……éƒ¨åˆ†
                - labels: åˆ†è¯åçš„æ‘˜è¦æ–‡æœ¬IDåºåˆ—ï¼ˆæ¨¡å‹è®­ç»ƒç›®æ ‡ï¼‰
        """
        # è·å–æŒ‡å®šç´¢å¼•çš„æ ·æœ¬
        if hasattr(self.dataset, 'iloc'):  # pandas DataFrame
            item = self.dataset.iloc[idx]
        else:  # Hugging Face Dataset
            item = self.dataset[idx]
        # item = self.dataset[idx]

        # ç¼–ç å¯¹è¯æ–‡æœ¬ï¼ˆæ¨¡å‹è¾“å…¥ï¼‰
        input_encoding = self.tokenizer(
            item['dialogue'],  # åŸå§‹å¯¹è¯æ–‡æœ¬
            max_length=self.max_input_length,  # æœ€å¤§åºåˆ—é•¿åº¦
            padding='max_length',  # å¡«å……åˆ°æœ€å¤§é•¿åº¦ï¼ˆä¿è¯æ‰¹æ¬¡å†…é•¿åº¦ä¸€è‡´ï¼‰
            truncation=True,  # å¯ç”¨æˆªæ–­ï¼ˆè¶…é•¿æ–‡æœ¬ä¼šè¢«æˆªæ–­ï¼‰
            return_tensors='pt'  # è¿”å›PyTorchå¼ é‡æ ¼å¼
        )
        # å‚æ•°è¯´æ˜ï¼š
        # - max_length: æ§åˆ¶åºåˆ—é•¿åº¦ï¼Œé¿å…å†…å­˜æº¢å‡º
        # - padding='max_length': å°†æ‰€æœ‰åºåˆ—å¡«å……åˆ°ç›¸åŒé•¿åº¦ï¼Œä¾¿äºæ‰¹æ¬¡å¤„ç†
        # - truncation=True: è‡ªåŠ¨æˆªæ–­è¶…é•¿æ–‡æœ¬
        # - return_tensors='pt': è¿”å›PyTorchå¼ é‡ï¼Œè€Œä¸æ˜¯Pythonåˆ—è¡¨

        # ç¼–ç æ‘˜è¦æ–‡æœ¬ï¼ˆæ¨¡å‹è®­ç»ƒç›®æ ‡ï¼‰
        target_encoding = self.tokenizer(
            item['summary'],  # åŸå§‹æ‘˜è¦æ–‡æœ¬
            max_length=self.max_target_length,  # æ‘˜è¦é€šå¸¸æ¯”å¯¹è¯çŸ­
            padding='max_length',  # åŒæ ·éœ€è¦å¡«å……åˆ°ç»Ÿä¸€é•¿åº¦
            truncation=True,  # å¯ç”¨æˆªæ–­
            return_tensors='pt'  # è¿”å›PyTorchå¼ é‡
        )

        # è¿”å›é¢„å¤„ç†åçš„æ ·æœ¬
        return {
            # å½¢çŠ¶ï¼š(seq_len,)ï¼Œå› ä¸ºtokenizerè¿”å›çš„æ˜¯ (1, seq_len)ï¼Œéœ€è¦ç”¨ .squeeze() ç§»é™¤è‡ªåŠ¨æ·»åŠ çš„æ‰¹æ¬¡ç»´åº¦ã€‚
            'input_ids': input_encoding['input_ids'].squeeze(),  # ç§»é™¤æ‰¹æ¬¡ç»´åº¦ï¼š(1, seq_len) -> (seq_len,)
            'attention_mask': input_encoding['attention_mask'].squeeze(),  # æ³¨æ„åŠ›æ©ç 
            'labels': target_encoding['input_ids'].squeeze()  # è®­ç»ƒæ ‡ç­¾ï¼ˆæ‘˜è¦çš„token IDï¼‰
        }
        # å­—æ®µè¯´æ˜ï¼š
        # - input_ids: å¯¹è¯æ–‡æœ¬è½¬æ¢åçš„æ•°å­—åºåˆ—ï¼Œå½¢çŠ¶ä¸º(seq_len,)
        # - attention_mask: åŒºåˆ†çœŸå®å†…å®¹å’Œå¡«å……éƒ¨åˆ†ï¼Œ1=çœŸå®å†…å®¹ï¼Œ0=å¡«å……éƒ¨åˆ†
        # - labels: æ‘˜è¦æ–‡æœ¬çš„æ•°å­—åºåˆ—ï¼Œæ¨¡å‹éœ€è¦å­¦ä¹ ç”Ÿæˆè¿™ä¸ªåºåˆ—


def load_local_samsum_dataset(data_dir="../data/samsum"):
    """
    ä»æœ¬åœ°CSVæ–‡ä»¶åŠ è½½SAMSumæ•°æ®é›†

    Args:
        data_dir: æœ¬åœ°æ•°æ®ç›®å½•è·¯å¾„ï¼ŒåŒ…å«train.csv, validation.csv, test.csv

    Returns:
        DatasetDict: åŒ…å«trainã€validationã€teståˆ†å‰²çš„æ•°æ®é›†å­—å…¸
    """
    # æ£€æŸ¥æ•°æ®ç›®å½•æ˜¯å¦å­˜åœ¨
    if not os.path.exists(data_dir):
        raise FileNotFoundError(f"æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}")

    # å®šä¹‰CSVæ–‡ä»¶è·¯å¾„
    train_path = os.path.join(data_dir, "train.csv")
    val_path = os.path.join(data_dir, "validation.csv")
    test_path = os.path.join(data_dir, "test.csv")

    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    for path in [train_path, val_path, test_path]:
        if not os.path.exists(path):
            raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {path}")

    print("ğŸ“ ä»æœ¬åœ°CSVæ–‡ä»¶åŠ è½½SAMSumæ•°æ®é›†...")

    # è¯»å–CSVæ–‡ä»¶
    train_df = pd.read_csv(train_path)
    val_df = pd.read_csv(val_path)
    test_df = pd.read_csv(test_path)

    print(f"è®­ç»ƒé›†å¤§å°: {len(train_df)}")
    print(f"éªŒè¯é›†å¤§å°: {len(val_df)}")
    print(f"æµ‹è¯•é›†å¤§å°: {len(test_df)}")

    # æ£€æŸ¥å¿…è¦çš„åˆ—æ˜¯å¦å­˜åœ¨
    required_columns = ['id', 'dialogue', 'summary']
    for df, split_name in [(train_df, 'è®­ç»ƒé›†'), (val_df, 'éªŒè¯é›†'), (test_df, 'æµ‹è¯•é›†')]:
        for col in required_columns:
            if col not in df.columns:
                raise ValueError(f"{split_name}ç¼ºå°‘å¿…è¦åˆ—: {col}")

    # è½¬æ¢ä¸ºHugging Face Datasetæ ¼å¼
    # train_dataset = Dataset.from_pandas(train_df)
    # val_dataset = Dataset.from_pandas(val_df)
    # test_dataset = Dataset.from_pandas(test_df)

    # åˆ›å»ºDatasetDict
    # ç›´æ¥è¿”å›pandas DataFrameï¼Œä¸ä½¿ç”¨Dataset.from_pandas
    dataset_dict = DatasetDict({
        'train': train_df,
        'validation': val_df,
        'test': test_df
    })
    # dataset_dict = DatasetDict({
    #     'train': train_dataset,
    #     'validation': val_dataset,
    #     'test': test_dataset
    # })

    return dataset_dict


def load_local_tokenizer(tokenizer_path="../data/tokenizer/facebook/bart-base"):
    """
    ä»æœ¬åœ°åŠ è½½åˆ†è¯å™¨ï¼Œæ”¯æŒå¤šç§å›é€€æ–¹æ¡ˆ
    Args:
        tokenizer_path: æœ¬åœ°åˆ†è¯å™¨è·¯å¾„
    Returns:
        tokenizer: åˆ†è¯å™¨å®ä¾‹
    """
    if os.path.exists(tokenizer_path):
        try:
            print(f"ğŸ” å°è¯•ä»æœ¬åœ°åŠ è½½åˆ†è¯å™¨: {tokenizer_path}")
            tokenizer = AutoTokenizer.from_pretrained(
                tokenizer_path,
                local_files_only=True  # å¼ºåˆ¶ä½¿ç”¨æœ¬åœ°æ–‡ä»¶
            )
            print("âœ… æˆåŠŸä»æœ¬åœ°åŠ è½½åˆ†è¯å™¨")

            # ç¡®ä¿æœ‰å¿…è¦çš„ç‰¹æ®Štoken
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
            if tokenizer.bos_token is None:
                tokenizer.bos_token = tokenizer.eos_token

            # print(f"åˆ†è¯å™¨è¯æ±‡è¡¨å¤§å°: {len(tokenizer)}")
            # print(f"å¡«å……token ID: {tokenizer.pad_token_id}")
            return tokenizer
        except Exception as e:
            print(f"âŒ æœ¬åœ°åŠ è½½å¤±è´¥ {tokenizer_path}: {e}")

    # å¦‚æœæœ¬åœ°è·¯å¾„å¤±è´¥ï¼Œå°è¯•åœ¨çº¿ä¸‹è½½ï¼ˆä½†è®¾ç½®è¶…æ—¶ï¼‰
    print("ğŸ”„ å°è¯•åœ¨çº¿ä¸‹è½½åˆ†è¯å™¨...")
    try:
        import socket
        socket.setdefaulttimeout(30)  # è®¾ç½®30ç§’è¶…æ—¶

        tokenizer = AutoTokenizer.from_pretrained('facebook/bart-base')
        print("âœ… åœ¨çº¿ä¸‹è½½åˆ†è¯å™¨æˆåŠŸ")

        # print(f"åˆ†è¯å™¨è¯æ±‡è¡¨å¤§å°: {len(tokenizer)}")
        # print(f"å¡«å……token ID: {tokenizer.pad_token_id}")

        # è‡ªåŠ¨ä¿å­˜åˆ°æœ¬åœ°ä¾›ä¸‹æ¬¡ä½¿ç”¨
        os.makedirs("../data/tokenizer/facebook/bart-base", exist_ok=True)
        tokenizer.save_pretrained("../data/tokenizer/facebook/bart-base")
        print("ğŸ’¾ åˆ†è¯å™¨å·²ä¿å­˜åˆ°æœ¬åœ°ç¼“å­˜")

        return tokenizer

    except Exception as e:
        print(f"âŒ åœ¨çº¿ä¸‹è½½ä¹Ÿå¤±è´¥: {e}")

def get_data_loaders(config):
    """
    åˆ›å»ºè®­ç»ƒå’ŒéªŒè¯æ•°æ®åŠ è½½å™¨
    è¿™ä¸ªå‡½æ•°è´Ÿè´£å®Œæ•´çš„æ•°æ®å¤„ç†æµç¨‹ï¼š
    1. åŠ è½½åŸå§‹æ•°æ®é›†
    2. åŠ è½½åˆ†è¯å™¨
    3. åˆ›å»ºDatasetå®ä¾‹
    4. åˆ›å»ºDataLoaderå®ä¾‹
    Args:
        config: é…ç½®å¯¹è±¡ï¼ŒåŒ…å«ä»¥ä¸‹å±æ€§ï¼š
            - dataset_name: æ•°æ®é›†åç§°ï¼ˆå¦‚"knkarthick/samsum"ï¼‰
            - batch_size: æ‰¹æ¬¡å¤§å°
            - max_input_length: è¾“å…¥åºåˆ—æœ€å¤§é•¿åº¦
            - max_target_length: ç›®æ ‡åºåˆ—æœ€å¤§é•¿åº¦

    Returns:
        tuple: (train_loader, val_loader, tokenizer)
            - train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            - val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
            - tokenizer: åˆ†è¯å™¨å®ä¾‹ï¼ˆåç»­ç”¨äºæ¨¡å‹å’Œæ¨ç†ï¼‰
    """

    print("ğŸš« å®Œå…¨ä½¿ç”¨æœ¬åœ°æ•°æ®ï¼Œé¿å…ç½‘ç»œè¿æ¥...")

    # 1. ä»æœ¬åœ°åŠ è½½æ•°æ®é›†
    try:
        dataset = load_local_samsum_dataset(config.local_data_dir)
        print("âœ… æˆåŠŸä»æœ¬åœ°æ–‡ä»¶åŠ è½½æ•°æ®é›†")
    except Exception as e:
        print(f"âŒ ä»æœ¬åœ°æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")

    # print(dataset)
    '''
    è¿”å›çš„datasetæ˜¯ä¸€ä¸ªDatasetDictå¯¹è±¡ï¼ŒåŒ…å«'train', 'validation', 'test'
    DatasetDict({
        train: Dataset({
            features: ['id', 'dialogue', 'summary'],
            num_rows: 14731
        })
        validation: Dataset({
            features: ['id', 'dialogue', 'summary'],
            num_rows: 818
        })
        test: Dataset({
            features: ['id', 'dialogue', 'summary'],
            num_rows: 819
        })
    })
    '''

    # 2. ä»æœ¬åœ°åŠ è½½åˆ†è¯å™¨
    tokenizer = load_local_tokenizer(config.local_tokenizer_path)
    # åˆ†è¯å™¨åŠŸèƒ½ï¼š
    # - å°†æ–‡æœ¬è½¬æ¢ä¸ºtoken IDåºåˆ—
    # - æ·»åŠ ç‰¹æ®Štokenï¼ˆå¦‚[CLS], [SEP], [PAD]ç­‰ï¼‰
    # - å¤„ç†å¡«å……å’Œæˆªæ–­

    # 3. åˆ›å»ºè®­ç»ƒé›†Datasetå®ä¾‹
    train_dataset = SAMSumDataset(
        dataset['train'],  # è®­ç»ƒé›†æ•°æ®
        tokenizer,  # åˆ†è¯å™¨
        config.max_input_length,  # è¾“å…¥åºåˆ—æœ€å¤§é•¿åº¦
        config.max_target_length  # ç›®æ ‡åºåˆ—æœ€å¤§é•¿åº¦
    )
    # æ­¤æ—¶train_datasetå·²ç»åŒ…å«äº†æ‰€æœ‰é¢„å¤„ç†é€»è¾‘

    # 4. åˆ›å»ºéªŒè¯é›†Datasetå®ä¾‹
    val_dataset = SAMSumDataset(
        dataset['validation'],  # éªŒè¯é›†æ•°æ®
        tokenizer,  # ä½¿ç”¨åŒä¸€ä¸ªåˆ†è¯å™¨ï¼ˆä¿è¯è¯æ±‡è¡¨ä¸€è‡´ï¼‰
        config.max_input_length,
        config.max_target_length
    )

    # 5. åˆ›å»ºæµ‹è¯•é›†Datasetå®ä¾‹
    test_dataset = SAMSumDataset(
        dataset['test'],  # æµ‹è¯•é›†æ•°æ®
        tokenizer,  # ä½¿ç”¨åŒä¸€ä¸ªåˆ†è¯å™¨ï¼ˆä¿è¯è¯æ±‡è¡¨ä¸€è‡´ï¼‰
        config.max_input_length,
        config.max_target_length
    )

    # 6. åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼ˆDataLoaderï¼‰
    # è®­ç»ƒæ•°æ®åŠ è½½å™¨ - æ”¯æŒéšæœºæ‰“ä¹±
    train_loader = DataLoader(
        train_dataset,  # è®­ç»ƒæ•°æ®é›†
        batch_size=config.batch_size,  # æ¯ä¸ªæ‰¹æ¬¡çš„æ ·æœ¬æ•°
        shuffle=True,  # æ¯ä¸ªepochæ‰“ä¹±æ•°æ®é¡ºåºï¼ˆé˜²æ­¢æ¨¡å‹è®°å¿†é¡ºåºï¼‰
        num_workers=4,# æ•°æ®åŠ è½½çš„è¿›ç¨‹æ•°ï¼ˆåŠ é€Ÿæ•°æ®åŠ è½½ï¼‰
        pin_memory=True,# æ˜¯å¦å°†æ•°æ®å›ºå®šåˆ°GPUå†…å­˜ï¼ˆåŠ é€ŸGPUä¼ è¾“ï¼‰
    )
    # è®­ç»ƒæ—¶æ‰“ä¹±æ•°æ®æœ‰åŠ©äºï¼š
    # - é˜²æ­¢æ¨¡å‹è¿‡æ‹Ÿåˆäºæ•°æ®é¡ºåº
    # - æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›

    # éªŒè¯æ•°æ®åŠ è½½å™¨ - ä¸éœ€è¦æ‰“ä¹±
    val_loader = DataLoader(
        val_dataset,  # éªŒè¯æ•°æ®é›†
        batch_size=config.batch_size,  # æ‰¹æ¬¡å¤§å°ï¼ˆé€šå¸¸ä¸è®­ç»ƒé›†ç›¸åŒï¼‰
        shuffle=False,  # éªŒè¯æ—¶ä¸æ‰“ä¹±æ•°æ®ï¼ˆä¿è¯è¯„ä¼°çš„ä¸€è‡´æ€§ï¼‰
        num_workers=4,
        pin_memory=True,
    )
    # éªŒè¯æ—¶ä¸æ‰“ä¹±çš„åŸå› ï¼š
    # - ä¿è¯æ¯æ¬¡éªŒè¯çš„ç»“æœå¯æ¯”è¾ƒ
    # - ä¾¿äºè°ƒè¯•å’Œç»“æœåˆ†æ

    test_loader = DataLoader(
        test_dataset,
        batch_size=config.batch_size,
        shuffle=False,  # æµ‹è¯•é›†ç»å¯¹ä¸æ‰“ä¹±
        num_workers=4,
        pin_memory=True,
    )

    # è¿”å›åˆ›å»ºå¥½çš„æ•°æ®åŠ è½½å™¨å’Œåˆ†è¯å™¨
    return train_loader, val_loader,test_loader, tokenizer

'''åå°å¼€VPNæ‰èƒ½åœ¨çº¿ä¸‹è½½æ•°æ®é›†å’Œåˆ†è¯å™¨ï¼Œå¦‚æœä¸å¼€è¯·æ³¨é‡Šæœ¬å‡½æ•°ï¼Œå–æ¶ˆå¦ä¸€ä¸ªget_data_loaderså‡½æ•°çš„æ³¨é‡Š'''
# def get_data_loaders(config):
#     """
#     åˆ›å»ºè®­ç»ƒå’ŒéªŒè¯æ•°æ®åŠ è½½å™¨
#     è¿™ä¸ªå‡½æ•°è´Ÿè´£å®Œæ•´çš„æ•°æ®å¤„ç†æµç¨‹ï¼š
#     1. åŠ è½½åŸå§‹æ•°æ®é›†
#     2. åŠ è½½åˆ†è¯å™¨
#     3. åˆ›å»ºDatasetå®ä¾‹
#     4. åˆ›å»ºDataLoaderå®ä¾‹
#     Args:
#         config: é…ç½®å¯¹è±¡ï¼ŒåŒ…å«ä»¥ä¸‹å±æ€§ï¼š
#             - dataset_name: æ•°æ®é›†åç§°ï¼ˆå¦‚"knkarthick/samsum"ï¼‰
#             - batch_size: æ‰¹æ¬¡å¤§å°
#             - max_input_length: è¾“å…¥åºåˆ—æœ€å¤§é•¿åº¦
#             - max_target_length: ç›®æ ‡åºåˆ—æœ€å¤§é•¿åº¦
#
#     Returns:
#         tuple: (train_loader, val_loader, tokenizer)
#             - train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
#             - val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
#             - tokenizer: åˆ†è¯å™¨å®ä¾‹ï¼ˆåç»­ç”¨äºæ¨¡å‹å’Œæ¨ç†ï¼‰
#     """
#
#     # 1. åŠ è½½æ•°æ®é›†
#     # ä½¿ç”¨Hugging Faceçš„load_datasetå‡½æ•°ä»Hubä¸‹è½½æˆ–åŠ è½½æœ¬åœ°æ•°æ®é›†
#     dataset = load_dataset(config.dataset_name)
#
#     # print(dataset)
#     '''
#     è¿”å›çš„datasetæ˜¯ä¸€ä¸ªDatasetDictå¯¹è±¡ï¼ŒåŒ…å«'train', 'validation', 'test'
#     DatasetDict({
#         train: Dataset({
#             features: ['id', 'dialogue', 'summary'],
#             num_rows: 14731
#         })
#         validation: Dataset({
#             features: ['id', 'dialogue', 'summary'],
#             num_rows: 818
#         })
#         test: Dataset({
#             features: ['id', 'dialogue', 'summary'],
#             num_rows: 819
#         })
#     })
#     '''
#
#
#     # 2. åŠ è½½åˆ†è¯å™¨
#     # ä½¿ç”¨BARTæ¨¡å‹çš„åˆ†è¯å™¨ï¼Œå› ä¸ºBARTåœ¨æ‘˜è¦ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜ç§€
#     tokenizer = AutoTokenizer.from_pretrained('facebook/bart-base')
#     # åˆ†è¯å™¨åŠŸèƒ½ï¼š
#     # - å°†æ–‡æœ¬è½¬æ¢ä¸ºtoken IDåºåˆ—
#     # - æ·»åŠ ç‰¹æ®Štokenï¼ˆå¦‚[CLS], [SEP], [PAD]ç­‰ï¼‰
#     # - å¤„ç†å¡«å……å’Œæˆªæ–­
#
#     # 3. åˆ›å»ºè®­ç»ƒé›†Datasetå®ä¾‹
#     train_dataset = SAMSumDataset(
#         dataset['train'],  # è®­ç»ƒé›†æ•°æ®
#         tokenizer,  # åˆ†è¯å™¨
#         config.max_input_length,  # è¾“å…¥åºåˆ—æœ€å¤§é•¿åº¦
#         config.max_target_length  # ç›®æ ‡åºåˆ—æœ€å¤§é•¿åº¦
#     )
#     # æ­¤æ—¶train_datasetå·²ç»åŒ…å«äº†æ‰€æœ‰é¢„å¤„ç†é€»è¾‘
#
#     # 4. åˆ›å»ºéªŒè¯é›†Datasetå®ä¾‹
#     val_dataset = SAMSumDataset(
#         dataset['validation'],  # éªŒè¯é›†æ•°æ®
#         tokenizer,  # ä½¿ç”¨åŒä¸€ä¸ªåˆ†è¯å™¨ï¼ˆä¿è¯è¯æ±‡è¡¨ä¸€è‡´ï¼‰
#         config.max_input_length,
#         config.max_target_length
#     )
#
#     # 5. åˆ›å»ºæµ‹è¯•é›†Datasetå®ä¾‹
#     test_dataset = SAMSumDataset(
#         dataset['test'],  # æµ‹è¯•é›†æ•°æ®
#         tokenizer,  # ä½¿ç”¨åŒä¸€ä¸ªåˆ†è¯å™¨ï¼ˆä¿è¯è¯æ±‡è¡¨ä¸€è‡´ï¼‰
#         config.max_input_length,
#         config.max_target_length
#     )
#
#     # 6. åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼ˆDataLoaderï¼‰
#     # è®­ç»ƒæ•°æ®åŠ è½½å™¨ - æ”¯æŒéšæœºæ‰“ä¹±
#     train_loader = DataLoader(
#         train_dataset,  # è®­ç»ƒæ•°æ®é›†
#         batch_size=config.batch_size,  # æ¯ä¸ªæ‰¹æ¬¡çš„æ ·æœ¬æ•°
#         shuffle=True,  # æ¯ä¸ªepochæ‰“ä¹±æ•°æ®é¡ºåºï¼ˆé˜²æ­¢æ¨¡å‹è®°å¿†é¡ºåºï¼‰
#         num_workers=4,# æ•°æ®åŠ è½½çš„è¿›ç¨‹æ•°ï¼ˆåŠ é€Ÿæ•°æ®åŠ è½½ï¼‰
#         pin_memory=True,# æ˜¯å¦å°†æ•°æ®å›ºå®šåˆ°GPUå†…å­˜ï¼ˆåŠ é€ŸGPUä¼ è¾“ï¼‰
#     )
#     # è®­ç»ƒæ—¶æ‰“ä¹±æ•°æ®æœ‰åŠ©äºï¼š
#     # - é˜²æ­¢æ¨¡å‹è¿‡æ‹Ÿåˆäºæ•°æ®é¡ºåº
#     # - æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›
#
#     # éªŒè¯æ•°æ®åŠ è½½å™¨ - ä¸éœ€è¦æ‰“ä¹±
#     val_loader = DataLoader(
#         val_dataset,  # éªŒè¯æ•°æ®é›†
#         batch_size=config.batch_size,  # æ‰¹æ¬¡å¤§å°ï¼ˆé€šå¸¸ä¸è®­ç»ƒé›†ç›¸åŒï¼‰
#         shuffle=False,  # éªŒè¯æ—¶ä¸æ‰“ä¹±æ•°æ®ï¼ˆä¿è¯è¯„ä¼°çš„ä¸€è‡´æ€§ï¼‰
#         num_workers=4,
#         pin_memory=True,
#     )
#     # éªŒè¯æ—¶ä¸æ‰“ä¹±çš„åŸå› ï¼š
#     # - ä¿è¯æ¯æ¬¡éªŒè¯çš„ç»“æœå¯æ¯”è¾ƒ
#     # - ä¾¿äºè°ƒè¯•å’Œç»“æœåˆ†æ
#
#     test_loader = DataLoader(
#         test_dataset,
#         batch_size=config.batch_size,
#         shuffle=False,  # æµ‹è¯•é›†ç»å¯¹ä¸æ‰“ä¹±
#         num_workers=4,
#         pin_memory=True,
#     )
#
#     # è¿”å›åˆ›å»ºå¥½çš„æ•°æ®åŠ è½½å™¨å’Œåˆ†è¯å™¨
#     return train_loader, val_loader,test_loader, tokenizer

# train_loader, val_loader,test_loader, tokenizer = get_data_loaders(config)
