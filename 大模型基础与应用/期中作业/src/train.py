import math
import os

import torch
import torch.nn as nn
from torch.optim import AdamW
from torch.optim.lr_scheduler import CosineAnnealingLR
import matplotlib.pyplot as plt
from tqdm import tqdm
import numpy as np
from rouge_score import rouge_scorer

from model import Transformer
from data_loader import get_data_loaders
from config import config
from utils import set_seed

import warnings
warnings.filterwarnings("ignore")# å¿½ç•¥è­¦å‘Šè¾“å‡º
plt.rcParams["font.sans-serif"] = ["SimHei"]# è®¾ç½®matplotlibä¸­æ–‡å­—ä½“æ˜¾ç¤ºï¼ˆè§£å†³ä¸­æ–‡ä¹±ç é—®é¢˜ï¼‰
plt.rcParams["axes.unicode_minus"] = False# æ˜¾ç¤ºè´Ÿå·

class Trainer:
    def __init__(self, model, train_loader, val_loader,test_loader, tokenizer, config):
        """
        è®­ç»ƒå™¨åˆå§‹åŒ–
        Args:
            model: Transformeræ¨¡å‹å®ä¾‹
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
            tokenizer: åˆ†è¯å™¨
            config: é…ç½®å¯¹è±¡
        """
        self.model = model.to(config.device)# å°†æ¨¡å‹ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡ï¼ˆGPU/CPUï¼‰ï¼Œç¡®ä¿æ¨¡å‹å‚æ•°å’Œè®¡ç®—åœ¨æ­£ç¡®çš„ç¡¬ä»¶ä¸Šæ‰§è¡Œ
        self.train_loader = train_loader# å­˜å‚¨è®­ç»ƒæ•°æ®åŠ è½½å™¨ï¼Œç”¨äºè¿­ä»£è®­ç»ƒæ•°æ®æ‰¹æ¬¡
        self.val_loader = val_loader# å­˜å‚¨éªŒè¯æ•°æ®åŠ è½½å™¨ï¼Œç”¨äºåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¯„ä¼°æ¨¡å‹æ€§èƒ½
        self.test_loader = test_loader# å­˜å‚¨æµ‹è¯•æ•°æ®åŠ è½½å™¨ï¼Œç”¨äºåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¯„ä¼°æ¨¡å‹æ€§èƒ½
        self.tokenizer = tokenizer# å­˜å‚¨åˆ†è¯å™¨å®ä¾‹ï¼Œç”¨äºæ–‡æœ¬ç¼–ç å’Œè§£ç æ“ä½œ
        self.config = config# å­˜å‚¨é…ç½®å¯¹è±¡ï¼ŒåŒ…å«æ‰€æœ‰è®­ç»ƒè¶…å‚æ•°å’Œè®¾ç½®

        # ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
        # åˆå§‹åŒ–AdamWä¼˜åŒ–å™¨ï¼Œç»“åˆäº†Adamç®—æ³•çš„è‡ªé€‚åº”å­¦ä¹ ç‡å’Œæƒé‡è¡°å‡æ­£åˆ™åŒ–
        # model.parameters()è¿”å›æ¨¡å‹ä¸­æ‰€æœ‰éœ€è¦è®­ç»ƒçš„å‚æ•°
        # lrè®¾ç½®åˆå§‹å­¦ä¹ ç‡ï¼Œæ ¹æ®config.learning_rateç¡®å®š
        self.optimizer = AdamW(model.parameters(), lr=config.learning_rate)
        # åˆå§‹åŒ–ä½™å¼¦é€€ç«å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ŒæŒ‰ç…§ä½™å¼¦å‡½æ•°è°ƒæ•´å­¦ä¹ ç‡
        # T_maxå‚æ•°æŒ‡å®šä½™å¼¦å‘¨æœŸçš„é•¿åº¦ï¼ˆè¿™é‡Œè®¾ç½®ä¸ºæ€»è®­ç»ƒè½®æ•°ï¼‰
        self.scheduler = CosineAnnealingLR(self.optimizer, T_max=config.num_epochs)
        # åˆå§‹åŒ–äº¤å‰ç†µæŸå¤±å‡½æ•°ï¼Œç”¨äºè®¡ç®—æ¨¡å‹é¢„æµ‹ä¸çœŸå®æ ‡ç­¾ä¹‹é—´çš„å·®å¼‚
        # ignore_indexå‚æ•°æŒ‡å®šè¦å¿½ç•¥çš„æ ‡ç­¾ç´¢å¼•ï¼ˆè¿™é‡Œå¿½ç•¥å¡«å……tokençš„IDï¼‰
        # è¿™æ ·å¯ä»¥é¿å…å¡«å……ä½ç½®å¯¹æŸå¤±è®¡ç®—äº§ç”Ÿå½±å“
        self.criterion = nn.CrossEntropyLoss(ignore_index=self.tokenizer.pad_token_id)

        # è®­ç»ƒè®°å½•
        self.train_losses = []# åˆå§‹åŒ–è®­ç»ƒæŸå¤±è®°å½•åˆ—è¡¨ï¼Œç”¨äºå­˜å‚¨æ¯ä¸ªepochçš„å¹³å‡è®­ç»ƒæŸå¤±
        self.val_losses = []# åˆå§‹åŒ–éªŒè¯æŸå¤±è®°å½•åˆ—è¡¨ï¼Œç”¨äºå­˜å‚¨æ¯ä¸ªepochçš„éªŒè¯æŸå¤±
        self.rouge_scores = []# åˆå§‹åŒ–ROUGEåˆ†æ•°è®°å½•åˆ—è¡¨ï¼Œç”¨äºå­˜å‚¨æ¯ä¸ªepochçš„æ–‡æœ¬ç”Ÿæˆè´¨é‡è¯„ä¼°åˆ†æ•°
        # ROUGEï¼ˆRecall-Oriented Understudy for Gisting Evaluationï¼‰æ˜¯ä¸“é—¨ç”¨äºè¯„ä¼°è‡ªåŠ¨æ–‡æ‘˜å’Œæœºå™¨ç¿»è¯‘ç­‰æ–‡æœ¬ç”Ÿæˆä»»åŠ¡çš„è´¨é‡æŒ‡æ ‡ã€‚
        # ROUGE-1ï¼ˆåŸºäºå•è¯ï¼‰ï¼šæ¯”è¾ƒå•ä¸ªå•è¯çš„é‡å æƒ…å†µ
        # ROUGE-2ï¼ˆåŸºäºå•è¯å¯¹ï¼‰:æ¯”è¾ƒè¿ç»­ä¸¤ä¸ªå•è¯çš„ç»„åˆ
        # ROUGE-Lï¼ˆåŸºäºæœ€é•¿å…¬å…±å­åºåˆ—ï¼‰:# å¯»æ‰¾æœ€é•¿çš„è¿ç»­åŒ¹é…åºåˆ—
        self.train_perplexities = []  # æ–°å¢ï¼šè®­ç»ƒå›°æƒ‘åº¦
        self.val_perplexities = []  # æ–°å¢ï¼šéªŒè¯å›°æƒ‘åº¦
        self.train_accuracies = []  # æ–°å¢ï¼šè®­ç»ƒå‡†ç¡®ç‡
        self.val_accuracies = []  # æ–°å¢ï¼šéªŒè¯å‡†ç¡®ç‡

    def calculate_perplexity(self, loss):
        """
        è®¡ç®—å›°æƒ‘åº¦ï¼ˆPerplexityï¼‰
        å›°æƒ‘åº¦æ˜¯è¯­è¨€æ¨¡å‹ä¸­æœ€é‡è¦çš„è¯„ä¼°æŒ‡æ ‡ä¹‹ä¸€ï¼Œè¡¨ç¤ºæ¨¡å‹å¯¹æµ‹è¯•æ•°æ®çš„"å›°æƒ‘ç¨‹åº¦"ã€‚
        æ•°å­¦å…¬å¼ï¼šperplexity = exp(loss)
        Args:
            loss: äº¤å‰ç†µæŸå¤±å€¼
        Returns:
            float: å›°æƒ‘åº¦å€¼
        è§£é‡Šï¼š
        - å›°æƒ‘åº¦è¶Šä½ï¼Œè¯´æ˜æ¨¡å‹å¯¹æ•°æ®çš„é¢„æµ‹è¶Šå‡†ç¡®
        - å®Œç¾é¢„æµ‹ï¼šå›°æƒ‘åº¦ = 1ï¼ˆæŸå¤±ä¸º0æ—¶ï¼‰
        - éšæœºçŒœæµ‹ï¼šå›°æƒ‘åº¦ = è¯æ±‡è¡¨å¤§å°ï¼ˆæœ€å·®æƒ…å†µï¼‰
        """
        return math.exp(loss)

    def calculate_accuracy(self, logits, labels, ignore_index=-100):
        """
        è®¡ç®—å‡†ç¡®ç‡ï¼ˆAccuracyï¼‰:
        å‡†ç¡®ç‡è¡¡é‡æ¨¡å‹é¢„æµ‹æ­£ç¡®çš„tokenæ¯”ä¾‹ï¼Œæ˜¯ç›´è§‚çš„è¯„ä¼°æŒ‡æ ‡ã€‚
        Args:
            logits: æ¨¡å‹è¾“å‡ºï¼Œå½¢çŠ¶ä¸º(batch_size, seq_len, vocab_size)
            labels: çœŸå®æ ‡ç­¾ï¼Œå½¢çŠ¶ä¸º(batch_size, seq_len)
            ignore_index: è¦å¿½ç•¥çš„æ ‡ç­¾ç´¢å¼•ï¼ˆå¦‚å¡«å……tokenï¼‰
        Returns:
            float: å‡†ç¡®ç‡ï¼ˆ0.0åˆ°1.0ä¹‹é—´ï¼‰
        """
        # è·å–é¢„æµ‹ç»“æœï¼ˆæ¦‚ç‡æœ€å¤§çš„tokenï¼‰
        predictions = torch.argmax(logits, dim=-1)
        # åˆ›å»ºæœ‰æ•ˆtokençš„æ©ç ï¼ˆå¿½ç•¥å¡«å……tokenï¼‰
        valid_mask = (labels != ignore_index)
        # è®¡ç®—æ­£ç¡®é¢„æµ‹çš„æ•°é‡
        correct = (predictions == labels) & valid_mask
        correct_count = correct.sum().item()
        # è®¡ç®—æœ‰æ•ˆtokençš„æ€»æ•°
        total_valid = valid_mask.sum().item()
        # é¿å…é™¤é›¶é”™è¯¯
        if total_valid == 0:
            return 0.0
        accuracy = correct_count / total_valid
        return accuracy

    def train_epoch(self, epoch):
        """
        è®­ç»ƒä¸€ä¸ªå®Œæ•´çš„epochï¼ˆéå†æ•´ä¸ªè®­ç»ƒæ•°æ®é›†ä¸€æ¬¡ï¼‰
        Args:
            epoch (int): å½“å‰è®­ç»ƒè½®æ•°ï¼Œç”¨äºè¿›åº¦æ˜¾ç¤ºå’Œæ—¥å¿—è®°å½•
        Returns:
            floatï¼ˆavg_lossï¼‰: è¯¥epochçš„å¹³å‡è®­ç»ƒæŸå¤±ï¼Œç”¨äºç›‘æ§è®­ç»ƒæ•ˆæœ
        Note:
            ä¸€ä¸ªepochåŒ…å«å¯¹è®­ç»ƒæ•°æ®é›†ä¸­æ‰€æœ‰æ‰¹æ¬¡çš„å‰å‘ä¼ æ’­ã€æŸå¤±è®¡ç®—ã€åå‘ä¼ æ’­å’Œå‚æ•°æ›´æ–°
        """
        # å°†æ¨¡å‹è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
        # è¿™ä¼šå¯ç”¨dropoutã€batch normalizationç­‰è®­ç»ƒç‰¹æœ‰çš„è¡Œä¸º
        self.model.train()
        total_loss = 0# åˆå§‹åŒ–è¯¥epochçš„æ€»æŸå¤±ï¼Œç”¨äºåç»­è®¡ç®—å¹³å‡æŸå¤±
        total_perplexity = 0
        total_accuracy = 0
        total_batches = 0
        # åˆ›å»ºè¿›åº¦æ¡ï¼Œæ˜¾ç¤ºå½“å‰epochå’Œè®­ç»ƒè¿›åº¦ï¼›tqdmæä¾›äº†ç›´è§‚çš„è®­ç»ƒè¿›åº¦å¯è§†åŒ–
        progress_bar = tqdm(self.train_loader, desc=f'Epoch {epoch}')
        # éå†è®­ç»ƒæ•°æ®åŠ è½½å™¨ä¸­çš„æ¯ä¸ªæ‰¹æ¬¡
        for batch in progress_bar:
            # 1. æ•°æ®å‡†å¤‡é˜¶æ®µ - å°†æ•°æ®ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡ï¼ˆGPU/CPUï¼‰
            # å°†è¾“å…¥åºåˆ—çš„token IDç§»åŠ¨åˆ°æ¨¡å‹æ‰€åœ¨çš„è®¾å¤‡
            input_ids = batch['input_ids'].to(self.config.device)
            # æ³¨æ„åŠ›æ©ç ï¼Œæ ‡è¯†å“ªäº›ä½ç½®æ˜¯çœŸå®å†…å®¹ï¼ˆ1ï¼‰å’Œå¡«å……å†…å®¹ï¼ˆ0ï¼‰
            attention_mask = batch['attention_mask'].to(self.config.device)
            # ç›®æ ‡åºåˆ—ï¼ˆæ‘˜è¦æ–‡æœ¬ï¼‰çš„token IDï¼Œä½œä¸ºè®­ç»ƒæ ‡ç­¾
            labels = batch['labels'].to(self.config.device)

            # 2. å‰å‘ä¼ æ’­ï¼ˆä½¿ç”¨Teacher ForcingæŠ€æœ¯ï¼‰
            # Teacher Forcingï¼šä½¿ç”¨çœŸå®ç›®æ ‡åºåˆ—ä½œä¸ºè§£ç å™¨è¾“å…¥ï¼ŒåŠ é€Ÿæ”¶æ•›
            # labels[:, :-1] ç§»é™¤ç›®æ ‡åºåˆ—çš„æœ€åä¸€ä¸ªtokenï¼Œä½œä¸ºè§£ç å™¨è¾“å…¥
            # è¿™æ ·æ¨¡å‹å­¦ä¹ çš„æ˜¯æ ¹æ®å‰n-1ä¸ªtokené¢„æµ‹ç¬¬nä¸ªtoken
            outputs = self.model(input_ids, labels[:, :-1])
            # æ¨¡å‹è¾“å‡ºçš„åŸå§‹åˆ†æ•°ï¼ˆlogitsï¼‰ï¼Œå½¢çŠ¶ä¸º(batch_size, seq_len, vocab_size)
            logits = outputs['logits']

            # 3. æŸå¤±è®¡ç®— - äº¤å‰ç†µæŸå¤±ï¼šæ¯”è¾ƒæ¨¡å‹é¢„æµ‹ä¸çœŸå®æ ‡ç­¾çš„å·®å¼‚
            # logits.reshape(-1, logits.size(-1)),
            # å°†logitsé‡å¡‘ä¸ºäºŒç»´å¼ é‡ï¼š(batch_size * seq_len, vocab_size)
            # è¿™æ ·æ¯ä¸€è¡Œå¯¹åº”ä¸€ä¸ªä½ç½®çš„è¯æ±‡è¡¨æ¦‚ç‡åˆ†å¸ƒ
            # labels[:, 1:].reshape(-1):
            # å°†ç›®æ ‡åºåˆ—é‡å¡‘ä¸ºä¸€ç»´å¼ é‡ï¼š(batch_size * seq_len)
            # labels[:, 1:] ç§»é™¤ç›®æ ‡åºåˆ—çš„ç¬¬ä¸€ä¸ªtokenï¼ˆé€šå¸¸æ˜¯èµ·å§‹ç¬¦ï¼‰
            # å› ä¸ºæ¨¡å‹åº”è¯¥ä»ç¬¬äºŒä¸ªtokenå¼€å§‹é¢„æµ‹ï¼ˆç¬¬ä¸€ä¸ªtokenæ˜¯å·²çŸ¥çš„èµ·å§‹ç‚¹ï¼‰
            loss = self.criterion(
                logits.reshape(-1, logits.size(-1)),
                labels[:, 1:].reshape(-1)
            )

            # è®¡ç®—å‡†ç¡®ç‡
            accuracy = self.calculate_accuracy(
                logits,
                labels[:, 1:],
                ignore_index=self.tokenizer.pad_token_id
            )

            # 4. åå‘ä¼ æ’­å’Œå‚æ•°æ›´æ–°
            # æ¸…ç©ºä¸Šä¸€è½®è®¡ç®—çš„æ¢¯åº¦ï¼Œé˜²æ­¢æ¢¯åº¦ç´¯ç§¯
            self.optimizer.zero_grad()
            # åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦ï¼šæŸå¤±å‡½æ•°å¯¹æ¨¡å‹å‚æ•°çš„å¯¼æ•°
            loss.backward()

            # æ¢¯åº¦è£å‰ª - é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
            # å½“æ¢¯åº¦èŒƒæ•°è¶…è¿‡é˜ˆå€¼æ—¶ï¼Œå°†æ¢¯åº¦æŒ‰æ¯”ä¾‹ç¼©å°ã€‚è¿™æœ‰åŠ©äºè®­ç»ƒç¨³å®šæ€§ï¼Œç‰¹åˆ«æ˜¯å¤„ç†é•¿åºåˆ—æ—¶
            # è¿‡å¤§çš„æ¢¯åº¦ä¼šå¯¼è‡´å‚æ•°æ›´æ–°è¿‡å¤§ï¼Œè®­ç»ƒä¸ç¨³å®šç”šè‡³å‘æ•£
            # å‚æ•°1ï¼šè¦è£å‰ªçš„æ¨¡å‹å‚æ•°ï¼›å‚æ•°2ï¼šæœ€å¤§æ¢¯åº¦èŒƒæ•°é˜ˆå€¼
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.max_grad_norm)
            # æ ¹æ®æ¢¯åº¦æ›´æ–°æ¨¡å‹å‚æ•°
            self.optimizer.step()

            # 5. æŸå¤±è®°å½•å’Œè¿›åº¦æ›´æ–°
            # ç´¯åŠ è¯¥æ‰¹æ¬¡çš„æŸå¤±å€¼ï¼ˆ.item()å°†å¼ é‡è½¬æ¢ä¸ºPythonæ•°å€¼ï¼‰
            total_loss += loss.item()
            total_perplexity += self.calculate_perplexity(loss.item())
            total_accuracy += accuracy
            total_batches += 1
            # æ›´æ–°è¿›åº¦æ¡æ˜¾ç¤ºå½“å‰æ‰¹æ¬¡çš„æŸå¤±
            # progress_bar.set_postfix({'loss': loss.item()})
            progress_bar.set_postfix({
                'loss': f'{loss.item():.4f}',
                'acc': f'{accuracy:.4f}',
                'ppl': f'{self.calculate_perplexity(loss.item()):.2f}'
            })

        # è®¡ç®—è¯¥epochçš„å¹³å‡æŸå¤±ï¼ˆæ€»æŸå¤±é™¤ä»¥æ‰¹æ¬¡æ•°é‡ï¼‰
        avg_loss = total_loss / len(self.train_loader)
        avg_perplexity = total_perplexity / len(self.train_loader)
        avg_accuracy = total_accuracy / len(self.train_loader)
        # å°†å¹³å‡æŸå¤±è®°å½•åˆ°è®­ç»ƒå†å²ä¸­ï¼Œç”¨äºåç»­åˆ†æå’Œå¯è§†åŒ–
        self.train_losses.append(avg_loss)
        self.train_perplexities.append(avg_perplexity)
        self.train_accuracies.append(avg_accuracy)
        # è¿”å›å¹³å‡æŸå¤±ï¼Œä¾›å¤–éƒ¨ç›‘æ§è®­ç»ƒè¿›åº¦
        # return avg_loss
        return avg_loss, avg_perplexity, avg_accuracy

    def validate(self, epoch):
        """
        éªŒè¯æ¨¡å‹æ€§èƒ½ - åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡å‹è¡¨ç°ï¼Œå¹¶ä¿å­˜é¢„æµ‹ç»“æœ
        Args:
            epoch (int): å½“å‰è®­ç»ƒè½®æ•°ï¼Œç”¨äºè¿›åº¦æ˜¾ç¤ºå’Œæ—¥å¿—è®°å½•
        Returns:
            tuple: (avg_loss, rouge_scores)
                - avg_loss (float): å¹³å‡éªŒè¯æŸå¤±
                - rouge_scores (dict): åŒ…å«ROUGE-1ã€ROUGE-2ã€ROUGE-Låˆ†æ•°çš„å­—å…¸
        Note:
            éªŒè¯è¿‡ç¨‹ä¸æ›´æ–°æ¨¡å‹å‚æ•°ï¼Œä»…ç”¨äºè¯„ä¼°å’Œç›‘æ§è®­ç»ƒè¿›åº¦
            ä½¿ç”¨torch.no_grad()ç¦ç”¨æ¢¯åº¦è®¡ç®—ï¼ŒèŠ‚çœå†…å­˜å’Œè®¡ç®—èµ„æº
        """
        # å°†æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        # è¿™ä¼šç¦ç”¨dropoutã€batch normalizationç­‰è®­ç»ƒç‰¹æœ‰çš„è¡Œä¸ºï¼Œç¡®ä¿è¯„ä¼°ç»“æœçš„ä¸€è‡´æ€§
        self.model.eval()
        # åˆå§‹åŒ–ç´¯è®¡å˜é‡
        total_loss = 0  # ç´¯è®¡éªŒè¯æŸå¤±
        total_perplexity = 0
        total_accuracy = 0
        all_predictions = []  # å­˜å‚¨æ‰€æœ‰ç”Ÿæˆçš„æ‘˜è¦æ–‡æœ¬
        all_references = []  # å­˜å‚¨æ‰€æœ‰çœŸå®çš„æ‘˜è¦æ–‡æœ¬

        # ä½¿ç”¨torch.no_grad()ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼›ç¦ç”¨æ¢¯åº¦è®¡ç®—ï¼ŒèŠ‚çœå†…å­˜å¹¶åŠ é€Ÿæ¨ç†è¿‡ç¨‹
        with torch.no_grad():
            # ä½¿ç”¨tqdmè¿›åº¦æ¡éå†éªŒè¯é›†çš„æ‰€æœ‰æ‰¹æ¬¡
            for batch in tqdm(self.val_loader, desc='Validating'):
                # 1. æ•°æ®å‡†å¤‡ - å°†æ•°æ®ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
                input_ids = batch['input_ids'].to(self.config.device)  # è¾“å…¥åºåˆ—token IDs
                attention_mask = batch['attention_mask'].to(self.config.device)  # æ³¨æ„åŠ›æ©ç 
                labels = batch['labels'].to(self.config.device)  # ç›®æ ‡åºåˆ—token IDs

                # 2. å‰å‘ä¼ æ’­è®¡ç®—æŸå¤±ï¼ˆä½¿ç”¨Teacher Forcingï¼‰
                # è¾“å…¥ï¼šå¯¹è¯æ–‡æœ¬ï¼Œç›®æ ‡ï¼šæ‘˜è¦æ–‡æœ¬ï¼ˆå»æ‰æœ€åä¸€ä¸ªtokenï¼‰
                outputs = self.model(input_ids, labels[:, :-1])
                logits = outputs['logits']  # æ¨¡å‹è¾“å‡ºçš„åŸå§‹åˆ†æ•°

                # 3. è®¡ç®—éªŒè¯æŸå¤±
                loss = self.criterion(
                    # å°†logitsé‡å¡‘ä¸ºäºŒç»´ï¼š(batch_size * seq_len, vocab_size)
                    logits.reshape(-1, logits.size(-1)),
                    # å°†ç›®æ ‡åºåˆ—é‡å¡‘ä¸ºä¸€ç»´ï¼š(batch_size * seq_len)
                    # labels[:, 1:] ç§»é™¤ç¬¬ä¸€ä¸ªtokenï¼ˆé€šå¸¸æ˜¯èµ·å§‹ç¬¦ï¼‰
                    labels[:, 1:].reshape(-1)
                )

                # è®¡ç®—å‡†ç¡®ç‡
                accuracy = self.calculate_accuracy(
                    logits,
                    labels[:, 1:],
                    ignore_index=self.tokenizer.pad_token_id
                )

                total_loss += loss.item()  # ç´¯åŠ æ‰¹æ¬¡æŸå¤±
                total_perplexity += self.calculate_perplexity(loss.item())
                total_accuracy += accuracy

                # 4. ç”Ÿæˆé¢„æµ‹ï¼ˆç”¨äºROUGEè¯„ä¼°ï¼‰
                try:
                    # 4.1 åˆ›å»ºæºåºåˆ—æ©ç ï¼ˆå¿½ç•¥å¡«å……tokenï¼‰
                    src_mask = self.model._create_src_mask(input_ids)

                    # 4.2 ä½¿ç”¨æ¨¡å‹ç”Ÿæˆæ‘˜è¦ï¼ˆæ¨ç†æ¨¡å¼ï¼‰
                    predictions = self.model.generate(
                        input_ids,
                        src_mask=src_mask,
                        max_length=self.config.max_target_length
                    )

                    # 4.3 è§£ç é¢„æµ‹ç»“æœå’ŒçœŸå®æ ‡ç­¾
                    decoded_preds = self.tokenizer.batch_decode(
                        predictions.cpu(),  # å°†å¼ é‡ç§»åŠ¨åˆ°CPU
                        skip_special_tokens=True  # è·³è¿‡ç‰¹æ®Štokenï¼ˆå¦‚[CLS], [SEP]ï¼‰
                    )
                    decoded_labels = self.tokenizer.batch_decode(
                        labels.cpu(),  # å°†å¼ é‡ç§»åŠ¨åˆ°CPU
                        skip_special_tokens=True  # è·³è¿‡ç‰¹æ®Štoken
                    )

                    # 4.4 å­˜å‚¨ç»“æœç”¨äºåç»­è¯„ä¼°
                    all_predictions.extend(decoded_preds)  # æ·»åŠ ç”Ÿæˆçš„æ‘˜è¦
                    all_references.extend(decoded_labels)  # æ·»åŠ çœŸå®çš„æ‘˜è¦

                except Exception as e:
                    print(f"ç”Ÿæˆé¢„æµ‹æ—¶å‡ºé”™: {e}")
                    # å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨è´ªå¿ƒè§£ç 
                    try:
                        # ç›´æ¥ä½¿ç”¨æ¨¡å‹è¾“å‡ºè¿›è¡Œè´ªå¿ƒè§£ç 
                        pred_tokens = torch.argmax(logits, dim=-1)
                        decoded_preds = self.tokenizer.batch_decode(
                            pred_tokens.cpu(),
                            skip_special_tokens=True
                        )
                        # è§£ç çœŸå®æ ‡ç­¾
                        decoded_labels = self.tokenizer.batch_decode(
                            labels.cpu(),
                            skip_special_tokens=True
                        )
                        all_predictions.extend(decoded_preds)
                        all_references.extend(decoded_labels)
                    except Exception as e2:
                        print(f"å¤‡ç”¨æ–¹æ¡ˆä¹Ÿå¤±è´¥: {e2}")
                        continue# å¦‚æœå¤‡ç”¨æ–¹æ¡ˆä¹Ÿå¤±è´¥ï¼Œè·³è¿‡è¯¥æ‰¹æ¬¡

        # 5. è®¡ç®—ROUGEè¯„ä¼°åˆ†æ•°
        if len(all_predictions) > 0 and len(all_references) > 0:
            # æˆåŠŸç”Ÿæˆé¢„æµ‹ï¼Œè®¡ç®—ROUGEåˆ†æ•°
            rouge = self.calculate_rouge(all_predictions, all_references)
        else:
            # æ— æ³•ç”Ÿæˆæœ‰æ•ˆé¢„æµ‹ï¼Œä½¿ç”¨é»˜è®¤åˆ†æ•°
            rouge = {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0}
            print("è­¦å‘Šï¼šæ— æ³•è®¡ç®—ROUGEåˆ†æ•°ï¼Œé¢„æµ‹ç»“æœä¸ºç©º")

        # 6. è®¡ç®—å¹³å‡éªŒè¯æŸå¤±
        avg_loss = total_loss / len(self.val_loader)  # æ€»æŸå¤±é™¤ä»¥æ‰¹æ¬¡æ•°é‡
        avg_perplexity = total_perplexity / len(self.val_loader)
        avg_accuracy = total_accuracy / len(self.val_loader)

        # 7. è®°å½•éªŒè¯ç»“æœ
        self.val_losses.append(avg_loss)  # è®°å½•éªŒè¯æŸå¤±å†å²
        self.val_perplexities.append(avg_perplexity)
        self.val_accuracies.append(avg_accuracy)
        self.rouge_scores.append(rouge)  # è®°å½•ROUGEåˆ†æ•°å†å²

        # è¿”å›éªŒè¯ç»“æœ
        # return avg_loss, rouge
        return avg_loss, avg_perplexity, avg_accuracy, rouge

    def calculate_rouge(self, predictions, references):
        """
        è®¡ç®—ROUGEè¯„ä¼°åˆ†æ•° - ç”¨äºè¯„ä¼°ç”Ÿæˆæ‘˜è¦çš„è´¨é‡
        ROUGE (Recall-Oriented Understudy for Gisting Evaluation) æ˜¯
        è‡ªåŠ¨æ–‡æ‘˜å’Œæœºå™¨ç¿»è¯‘é¢†åŸŸæœ€å¸¸ç”¨çš„è¯„ä¼°æŒ‡æ ‡ï¼Œé€šè¿‡æ¯”è¾ƒç”Ÿæˆæ–‡æœ¬ä¸
        å‚è€ƒæ–‡æœ¬çš„n-gramé‡å åº¦æ¥è¯„ä¼°è´¨é‡ã€‚
        Args:
            predictions (list[str]): æ¨¡å‹ç”Ÿæˆçš„æ‘˜è¦æ–‡æœ¬åˆ—è¡¨
                - æ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œä»£è¡¨æ¨¡å‹å¯¹å•ä¸ªæ ·æœ¬çš„æ‘˜è¦ç”Ÿæˆç»“æœ
                - ç¤ºä¾‹: ["çŒ«åœ¨å«å­ä¸Šç¡è§‰", "ç‹—åœ¨é™¢å­é‡Œç©è€"]
            references (list[str]): äººå·¥æ’°å†™çš„å‚è€ƒæ‘˜è¦æ–‡æœ¬åˆ—è¡¨
                - æ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œä»£è¡¨è¯¥æ ·æœ¬çš„æ ‡å‡†ç­”æ¡ˆï¼ˆé»„é‡‘æ‘˜è¦ï¼‰
                - ç¤ºä¾‹: ["ä¸€åªçŒ«åœ¨å«å­ä¸Šä¼‘æ¯", "ç‹—åœ¨èŠ±å›­é‡Œç©"]
                - æ³¨æ„ï¼šå¿…é¡»ä¸predictionsåˆ—è¡¨é¡ºåºä¸€è‡´ä¸”é•¿åº¦ç›¸åŒ
        Returns:
            dict: åŒ…å«ä¸‰ç§ROUGEåˆ†æ•°çš„å­—å…¸
                - 'rouge1': ROUGE-1åˆ†æ•°ï¼ˆåŸºäºå•ä¸ªå•è¯ï¼‰
                - 'rouge2': ROUGE-2åˆ†æ•°ï¼ˆåŸºäºå•è¯å¯¹ï¼‰
                - 'rougeL': ROUGE-Låˆ†æ•°ï¼ˆåŸºäºæœ€é•¿å…¬å…±å­åºåˆ—ï¼‰
                - æ¯ä¸ªåˆ†æ•°éƒ½æ˜¯è¯¥æ‰¹æ¬¡æ‰€æœ‰æ ·æœ¬çš„å¹³å‡å€¼ï¼ŒèŒƒå›´[0.0, 1.0]
        Note:
            ROUGEåˆ†æ•°è¶Šé«˜è¡¨ç¤ºç”Ÿæˆæ‘˜è¦ä¸å‚è€ƒæ‘˜è¦è¶Šç›¸ä¼¼ï¼Œè´¨é‡è¶Šå¥½
            é€šå¸¸ROUGE-1 > ROUGE-L > ROUGE-2ï¼Œå› ä¸ºåŒ¹é…è¦æ±‚é€æ¸ä¸¥æ ¼
        """
        # åˆå§‹åŒ–ROUGEè¯„åˆ†å™¨
        # RougeScoreræ˜¯rouge_scoreåº“çš„æ ¸å¿ƒç±»ï¼Œè´Ÿè´£è®¡ç®—å„ç§ROUGEæŒ‡æ ‡
        # å‚æ•°è¯´æ˜ï¼š
        # - ['rouge1', 'rouge2', 'rougeL']: åŒæ—¶è®¡ç®—ä¸‰ç§ROUGEåˆ†æ•°
        # - use_stemmer=True: å¯¹å•è¯è¿›è¡Œè¯å¹²æå–ï¼ˆå¦‚"running"â†’"run"ï¼‰
        #   å‡å°‘è¯å½¢å˜åŒ–å¯¹åŒ¹é…çš„å½±å“ï¼Œæé«˜è¯„ä¼°çš„è¯­ä¹‰å‡†ç¡®æ€§
        scorer = rouge_scorer.RougeScorer(
            ['rouge1', 'rouge2', 'rougeL'],  # æŒ‡å®šè¦è®¡ç®—çš„ROUGEç±»å‹
            use_stemmer=True  # å¯ç”¨è¯å¹²æå–ï¼Œæé«˜åŒ¹é…å‡†ç¡®æ€§
        )
        # åˆå§‹åŒ–åˆ†æ•°å­˜å‚¨åˆ—è¡¨
        rouge1_scores = []  # å­˜å‚¨æ¯ä¸ªæ ·æœ¬çš„ROUGE-1åˆ†æ•°
        rouge2_scores = []  # å­˜å‚¨æ¯ä¸ªæ ·æœ¬çš„ROUGE-2åˆ†æ•°
        rougeL_scores = []  # å­˜å‚¨æ¯ä¸ªæ ·æœ¬çš„ROUGE-Låˆ†æ•°

        # éå†æ¯ä¸ªæ ·æœ¬å¯¹ï¼ˆç”Ÿæˆæ‘˜è¦ vs å‚è€ƒæ‘˜è¦ï¼‰
        for pred, ref in zip(predictions, references):
            # è®¡ç®—å½“å‰æ ·æœ¬çš„ROUGEåˆ†æ•°
            # scorer.score() è¿”å›åŒ…å«precision, recall, fmeasureçš„å­—å…¸
            scores = scorer.score(ref, pred)
            # # scorer.score()è¿”å›çš„ä¸‰ç§æŒ‡æ ‡ï¼š
            # scores = {
            #     'rouge1': {
            #         'precision': 0.75,   # ç²¾ç¡®ç‡ï¼šç”Ÿæˆæ‘˜è¦ä¸­æœ‰å¤šå°‘æ˜¯æ­£ç¡®çš„
            #         'recall': 0.60,      # å¬å›ç‡ï¼šå‚è€ƒæ‘˜è¦ä¸­æœ‰å¤šå°‘è¢«è¦†ç›–
            #         'fmeasure': 0.67     # F1åˆ†æ•°ï¼šç²¾ç¡®ç‡å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡
            #     }
            # }
            # æå–å¹¶å­˜å‚¨F1åˆ†æ•°ï¼ˆç²¾ç¡®ç‡å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡ï¼‰
            # F1åˆ†æ•°ç»¼åˆè€ƒè™‘äº†ç”Ÿæˆæ‘˜è¦çš„å‡†ç¡®æ€§å’Œå®Œæ•´æ€§
            rouge1_scores.append(scores['rouge1'].fmeasure)  # å•è¯çº§åˆ«ç›¸ä¼¼åº¦
            rouge2_scores.append(scores['rouge2'].fmeasure)  # çŸ­è¯­çº§åˆ«ç›¸ä¼¼åº¦
            rougeL_scores.append(scores['rougeL'].fmeasure)  # å¥å­ç»“æ„ç›¸ä¼¼åº¦

        # è®¡ç®—æ‰¹æ¬¡å¹³å‡åˆ†æ•°å¹¶è¿”å›
        return {
            'rouge1': np.mean(rouge1_scores),  # å¹³å‡ROUGE-1åˆ†æ•°
            'rouge2': np.mean(rouge2_scores),  # å¹³å‡ROUGE-2åˆ†æ•°
            'rougeL': np.mean(rougeL_scores)  # å¹³å‡ROUGE-Låˆ†æ•°
        }

    def train(self):
        """
        ä¸»è®­ç»ƒå¾ªç¯ - æ§åˆ¶æ•´ä¸ªæ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹
        è¿™ä¸ªæ–¹æ³•æ˜¯è®­ç»ƒå™¨çš„æ ¸å¿ƒï¼Œè´Ÿè´£ï¼š
        1. åˆå§‹åŒ–è®­ç»ƒç¯å¢ƒ
        2. æ‰§è¡Œå¤šä¸ªè®­ç»ƒepoch
        3. åœ¨æ¯ä¸ªepochåè¿›è¡ŒéªŒè¯è¯„ä¼°
        4. ä¿å­˜æœ€ä½³æ¨¡å‹å’Œæ£€æŸ¥ç‚¹
        5. è¾“å‡ºè®­ç»ƒç»“æœå’Œç»Ÿè®¡ä¿¡æ¯
        """
        print("å¼€å§‹è®­ç»ƒ...")
        print(f"è®¾å¤‡: {self.config.device}")
        print(f"è®­ç»ƒé›†å¤§å°: {len(self.train_loader.dataset)}")
        print(f"éªŒè¯é›†å¤§å°: {len(self.val_loader.dataset)}")
        print(f"æµ‹è¯•é›†å¤§å°: {len(self.test_loader.dataset)}")
        print(f"æ‰¹æ¬¡å¤§å°: {self.config.batch_size}")
        print(f"å­¦ä¹ ç‡: {self.config.learning_rate}")

        best_rouge = 0  # è®°å½•æœ€ä½³çš„ROUGE-1åˆ†æ•°
        best_accuracy = 0.0  # è®°å½•æœ€ä½³éªŒè¯å‡†ç¡®ç‡
        best_epoch = 0  # è®°å½•æœ€ä½³æ€§èƒ½å‡ºç°çš„epoch

        for epoch in range(self.config.num_epochs):
            print(f"\n{'=' * 50}")
            print(f"Epoch {epoch + 1}/{self.config.num_epochs}")
            print(f"{'=' * 50}")

            # è®­ç»ƒé˜¶æ®µ
            # æ‰§è¡Œä¸€ä¸ªå®Œæ•´çš„è®­ç»ƒepoch
            # ä½¿ç”¨epoch+1æ˜¯ä¸ºäº†æ˜¾ç¤ºæ›´ç›´è§‚çš„è¿›åº¦ï¼ˆä»1å¼€å§‹è€Œä¸æ˜¯0ï¼‰
            # train_loss = self.train_epoch(epoch+1)
            train_loss, train_ppl, train_acc = self.train_epoch(epoch + 1)

            # éªŒè¯é˜¶æ®µ
            # val_loss, rouge_scores = self.validate(epoch+1)
            val_loss, val_ppl, val_acc, rouge_scores = self.validate(epoch + 1)

            # æ ¹æ®å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥è°ƒæ•´å­¦ä¹ ç‡
            self.scheduler.step()  # æ›´æ–°å­¦ä¹ ç‡ï¼ˆå¦‚ä½™å¼¦é€€ç«ï¼‰
            current_lr = self.optimizer.param_groups[0]['lr']  # è·å–å½“å‰å­¦ä¹ ç‡

            # æ‰“å°ç»“æœ
            print(f"\nğŸ“Š è®­ç»ƒç»“æœ:")
            print(f"è®­ç»ƒæŸå¤±: {train_loss:.4f} | éªŒè¯æŸå¤±: {val_loss:.4f}")
            print(f"è®­ç»ƒå›°æƒ‘åº¦: {train_ppl:.2f} | éªŒè¯å›°æƒ‘åº¦: {val_ppl:.2f}")
            print(f"è®­ç»ƒå‡†ç¡®ç‡: {train_acc:.4f} | éªŒè¯å‡†ç¡®ç‡: {val_acc:.4f}")
            print(f"å­¦ä¹ ç‡: {current_lr:.2e}")
            print(f"ROUGE-1: {rouge_scores['rouge1']:.4f} | ROUGE-2: {rouge_scores['rouge2']:.4f} | ROUGE-L: {rouge_scores['rougeL']:.4f}")
            # print(f"\nè®­ç»ƒç»“æœ:")
            # print(f"è®­ç»ƒæŸå¤±: {train_loss:.4f}")  # è®­ç»ƒé›†æŸå¤±ï¼ˆè¶Šä½è¶Šå¥½ï¼‰
            # print(f"éªŒè¯æŸå¤±: {val_loss:.4f}")  # éªŒè¯é›†æŸå¤±ï¼ˆè¶Šä½è¶Šå¥½ï¼‰
            # print(f"å­¦ä¹ ç‡: {current_lr:.2e}")  # å½“å‰å­¦ä¹ ç‡ï¼ˆç§‘å­¦è®¡æ•°æ³•æ˜¾ç¤ºï¼‰
            # # ROUGEåˆ†æ•°è¾“å‡ºï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰
            # print(f"ROUGE-1: {rouge_scores['rouge1']:.4f}")  # å•è¯çº§åˆ«ç›¸ä¼¼åº¦
            # print(f"ROUGE-2: {rouge_scores['rouge2']:.4f}")  # çŸ­è¯­çº§åˆ«ç›¸ä¼¼åº¦
            # print(f"ROUGE-L: {rouge_scores['rougeL']:.4f}")  # å¥å­ç»“æ„ç›¸ä¼¼åº¦

            # ä¿å­˜æœ€ä½³æ¨¡å‹(åŸºäºå‡†ç¡®ç‡)
            if val_acc > best_accuracy:
                best_accuracy = val_acc
                best_epoch = epoch + 1  # è®°å½•æœ€ä½³epoch
                best_model_path = os.path.join(self.config.save_dir, f'best_model_{self.config.num_epochs}epochs_{self.config.n_heads}heads.pth')
                # ä¿å­˜å®Œæ•´çš„æ¨¡å‹æ£€æŸ¥ç‚¹
                torch.save({
                    'epoch': epoch,  # å½“å‰epochæ•°
                    'model_state_dict': self.model.state_dict(),  # æ¨¡å‹å‚æ•°
                    'optimizer_state_dict': self.optimizer.state_dict(),  # ä¼˜åŒ–å™¨çŠ¶æ€
                    'train_loss': train_loss,  # è®­ç»ƒæŸå¤±
                    'val_loss': val_loss,  # éªŒè¯æŸå¤±
                    'train_perplexity': train_ppl,
                    'val_perplexity': val_ppl,
                    'train_accuracy': train_acc,
                    'val_accuracy': val_acc,
                    'rouge_scores': rouge_scores  # ROUGEåˆ†æ•°
                }, best_model_path)  # ä¿å­˜ä¸ºbest_model.pthæ–‡ä»¶
                print(f"âœ… ä¿å­˜æ–°çš„æœ€ä½³æ¨¡å‹ (Epoch {best_epoch}, éªŒè¯å‡†ç¡®ç‡: {best_accuracy:.4f}, ä¿å­˜åœ°å€ï¼š{best_model_path})")
            # ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆåŸºäºROUGE-1åˆ†æ•°ï¼‰
            '''
            if rouge_scores['rouge1'] > best_rouge:
                best_rouge = rouge_scores['rouge1']  # æ›´æ–°æœ€ä½³åˆ†æ•°
                best_epoch = epoch + 1  # è®°å½•æœ€ä½³epoch
                best_model_path = os.path.join(self.config.save_dir, 'best_model.pth')
                # ä¿å­˜å®Œæ•´çš„æ¨¡å‹æ£€æŸ¥ç‚¹
                torch.save({
                    'epoch': epoch,  # å½“å‰epochæ•°
                    'model_state_dict': self.model.state_dict(),  # æ¨¡å‹å‚æ•°
                    'optimizer_state_dict': self.optimizer.state_dict(),  # ä¼˜åŒ–å™¨çŠ¶æ€
                    'train_loss': train_loss,  # è®­ç»ƒæŸå¤±
                    'val_loss': val_loss,  # éªŒè¯æŸå¤±
                    'train_perplexity': train_ppl,
                    'val_perplexity': val_ppl,
                    'train_accuracy': train_acc,
                    'val_accuracy': val_acc,
                    'rouge_scores': rouge_scores  # ROUGEåˆ†æ•°
                }, best_model_path)  # ä¿å­˜ä¸ºbest_model.pthæ–‡ä»¶
                print(f"âœ… ä¿å­˜æ–°çš„æœ€ä½³æ¨¡å‹ (Epoch {best_epoch}, ROUGE-1: {best_rouge:.4f}, ä¿å­˜åœ°å€ï¼š{best_model_path})")
            '''

            # æ¯5ä¸ªepochä¿å­˜ä¸€æ¬¡æ£€æŸ¥ç‚¹ï¼ˆé˜²æ­¢è®­ç»ƒä¸­æ–­ä¸¢å¤±è¿›åº¦ï¼‰
            if (epoch + 1) % 5 == 0:
                # æ£€æŸ¥ç‚¹æ–‡ä»¶å
                checkpoint_path = os.path.join(
                    self.config.checkpoint_dir,
                    f'checkpoint_epoch_{epoch + 1}.pth'
                )
                # checkpoint_path = f'../results/model/checkpoint_epoch_{epoch + 1}.pth'
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': self.model.state_dict(),
                    'optimizer_state_dict': self.optimizer.state_dict(),
                    'train_loss': train_loss,
                    'val_loss': val_loss,
                    'rouge_scores': rouge_scores
                }, checkpoint_path)
                print(f"ğŸ’¾ æ–°çš„æ¯äº”è½®ä¿å­˜æ£€æŸ¥ç‚¹: {checkpoint_path}")

        # è®­ç»ƒç»“æŸ
        print(f"\n{'=' * 50}")
        print("è®­ç»ƒå®Œæˆ!")
        print(f"{'=' * 50}")
        print(f"æœ€ä½³æ¨¡å‹åœ¨ Epoch {best_epoch}, ROUGE-1: {best_rouge:.4f}")

        # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
        self.plot_training_curves()

        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        torch.save(self.model.state_dict(), f'../results/model/final_model_{self.config.num_epochs}epochs_{self.config.n_heads}heads.pth')
        print(f"ğŸ’¾ ä¿å­˜æœ€ç»ˆæ¨¡å‹: ../results/model/final_model_{self.config.num_epochs}epochs_{self.config.n_heads}heads.pth")

    def plot_training_curves(self):
        """ç»˜åˆ¶å®Œæ•´çš„è®­ç»ƒæ›²çº¿ï¼ŒåŒ…å«æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡"""
        plt.figure(figsize=(20, 10))

        # 1. æŸå¤±æ›²çº¿
        plt.subplot(2, 2, 1)
        epochs = range(1, len(self.train_losses) + 1)
        plt.plot(epochs, self.train_losses, 'b-', label='è®­ç»ƒæŸå¤±', linewidth=2)
        plt.plot(epochs, self.val_losses, 'r-', label='éªŒè¯æŸå¤±', linewidth=2)
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.title('æŸå¤±æ›²çº¿')
        plt.legend()
        plt.grid(True, alpha=0.3)

        # 2. å›°æƒ‘åº¦æ›²çº¿
        plt.subplot(2, 2, 2)
        plt.plot(epochs, self.train_perplexities, 'b-', label='è®­ç»ƒå›°æƒ‘åº¦', linewidth=2)
        plt.plot(epochs, self.val_perplexities, 'r-', label='éªŒè¯å›°æƒ‘åº¦', linewidth=2)
        plt.xlabel('Epoch')
        plt.ylabel('Perplexity')
        plt.title('å›°æƒ‘åº¦æ›²çº¿')
        plt.legend()
        plt.grid(True, alpha=0.3)

        # 3. å‡†ç¡®ç‡æ›²çº¿
        plt.subplot(2, 2, 3)
        plt.plot(epochs, self.train_accuracies, 'b-', label='è®­ç»ƒå‡†ç¡®ç‡', linewidth=2)
        plt.plot(epochs, self.val_accuracies, 'r-', label='éªŒè¯å‡†ç¡®ç‡', linewidth=2)
        plt.xlabel('Epoch')
        plt.ylabel('Accuracy')
        plt.title('å‡†ç¡®ç‡æ›²çº¿')
        plt.legend()
        plt.grid(True, alpha=0.3)

        # 4. ROUGEåˆ†æ•°æ›²çº¿
        plt.subplot(2, 2, 4)
        rouge1_scores = [score['rouge1'] for score in self.rouge_scores]
        rouge2_scores = [score['rouge2'] for score in self.rouge_scores]
        rougeL_scores = [score['rougeL'] for score in self.rouge_scores]

        plt.plot(epochs, rouge1_scores, 'g-', label='ROUGE-1', linewidth=2)
        plt.plot(epochs, rouge2_scores, 'b-', label='ROUGE-2', linewidth=2)
        plt.plot(epochs, rougeL_scores, 'r-', label='ROUGE-L', linewidth=2)
        plt.xlabel('Epoch')
        plt.ylabel('ROUGE Score')
        plt.title('ROUGEåˆ†æ•°æ›²çº¿')
        plt.legend()
        plt.grid(True, alpha=0.3)

        plt.tight_layout()
        image_filename = f'training_curves_{self.config.num_epochs}epochs_{self.config.n_heads}heads.png'
        image_path = os.path.join(self.config.image_dir, image_filename)
        plt.savefig(image_path, dpi=300, bbox_inches='tight')
        plt.show()

        # æ‰“å°æœ€ç»ˆç»Ÿè®¡
        print(f"\nğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
        print(f"æœ€ç»ˆè®­ç»ƒæŸå¤±: {self.train_losses[-1]:.4f}")
        print(f"æœ€ç»ˆéªŒè¯æŸå¤±: {self.val_losses[-1]:.4f}")
        print(f"æœ€ç»ˆè®­ç»ƒå›°æƒ‘åº¦: {self.train_perplexities[-1]:.2f}")
        print(f"æœ€ç»ˆéªŒè¯å›°æƒ‘åº¦: {self.val_perplexities[-1]:.2f}")
        print(f"æœ€ç»ˆè®­ç»ƒå‡†ç¡®ç‡: {self.train_accuracies[-1]:.4f}")
        print(f"æœ€ç»ˆéªŒè¯å‡†ç¡®ç‡: {self.val_accuracies[-1]:.4f}")
        if self.rouge_scores:
            final_rouge = self.rouge_scores[-1]
            print(f"æœ€ç»ˆROUGE-1: {final_rouge['rouge1']:.4f}")
            print(f"æœ€ç»ˆROUGE-2: {final_rouge['rouge2']:.4f}")
            print(f"æœ€ç»ˆROUGE-L: {final_rouge['rougeL']:.4f}")


def main():
    """
    ä¸»å‡½æ•°
    """
    # è®¾ç½®éšæœºç§å­
    set_seed(config.seed)
    print("è®¾ç½®éšæœºç§å­å®Œæˆ")

    # è·å–æ•°æ®
    print("åŠ è½½æ•°æ®...")
    train_loader, val_loader,test_loader, tokenizer = get_data_loaders(config)

    # åˆ›å»ºæ¨¡å‹
    print("åˆå§‹åŒ–æ¨¡å‹...")
    # model = Transformer(config)
    model = Transformer(config, tokenizer=tokenizer)
    # æ‰“å°æ¨¡å‹å‚æ•°æ•°é‡
    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"æ¨¡å‹å¯è®­ç»ƒå‚æ•°æ•°é‡: {total_params:,}")

    # æµ‹è¯•ä¸åŒæ³¨æ„åŠ›å¤´æ•°é‡çš„å·®åˆ«,æ”¹ä¸º8ï¼Œè¦æµ‹è¯•å°±æŠŠä¸‹é¢è¿™è¡Œå–æ¶ˆæ³¨é‡Š
    # config.n_heads = 8

    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = Trainer(model, train_loader, val_loader,test_loader, tokenizer, config)

    # å¼€å§‹è®­ç»ƒ
    trainer.train()


if __name__ == '__main__':
    main()